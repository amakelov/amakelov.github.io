[{"content":"Introduction They finally figured out how the Roman Empire actually fell, and it was this file: model_alpha=0.1_best_VERSION2_preprocess=True_final_FIXED.pkl. \u0026hellip;OK, we all agree this filename\u0026rsquo;s terrible - but why exactly? Its job is to tell the story of a computation. However, computations are more expressive, structured, and malleable than filenames. So, it\u0026rsquo;s no wonder you run into awkwardness, confusion and bugs when you try to turn one into the other. This impedance mismatch is at the root of the computational data management problem.\nmandala: computations that save, load, and query themselves By contrast, the code producing the file\u0026rsquo;s contents is a far superior telling of its story:\nX, y = load_data() if preprocess: X = preprocess_data(X) model = train_v2(X, y, alpha=0.1) It makes it clear how logic and parameters combine to produce model. More subtly, implicit in the code are the invariant across executions computational relationships that we want to query (e.g., \u0026ldquo;find all the models trained with a particular dataset X\u0026rdquo;). If the code already fully describes what\u0026rsquo;s being computed, why can\u0026rsquo;t we just cut the (impedance-mismatched) middleman, and make the program its own storage interface?\nmandala is a tool that implements this idea. It turns function calls into composable, interlinked, queriable data that is automatically co-evolved with your codebase. By applying different semantics to this data, the same piece of ordinary Python code - possibly using control flow and collections - can be used to not only compute, but also save, load, query, delete, batch-compute, and combinations of those.\nCompared to existing frameworks addressing various parts of this problem, mandala bakes data management logic deeper into Python itself, and intentionally has few interfaces, concepts, and domain-specific features to speak of. Instead, it lets you do your work mostly through plain code. This direct approach makes it simpler to incrementally and interactively compute with and query experiments, especially in projects that compose many moving parts.\nOutline This blog post is a brief, somewhat programming-languages-themed tour of the unusual core features that work together to make this possible:\nmemoization aligned with core Python features to turn programs into imperative computation/storage/query interfaces and enable incremental computing; pattern-matching against code to turn programs into declarative query interfaces; interfaces to co-evolve code and data in a fine-grained way. All code examples used in this blog post are available in a notebook TODO.\nPython-integrated memoization While memoization is typically used as a mere cache, it can be much more. Imagine a complex piece of code where every function call has been memoized in a shared cache. Such code effectively becomes a map of the cache that you can traverse by re-execution, no matter how complex the logic. You can also incrementally build computations by just adding to this code: you get to the memoized part fast, and do the new work. This is especially useful in interactive environments like Jupyter notebooks.\nOrdinary memoization is not scalable enough for this, but it turns out that by baking memoization deeper into the language, you get a storage interface that is both practical and plain-Python.\nCollections, lazy reads, and control flow To illustrate, here\u0026rsquo;s a simple machine learning pipeline for a do-it-yourself random forest classifier:\n@op # memoization (\u0026amp; more) decorator def generate_data() -\u0026gt; Tuple[ndarray, ndarray]: ... @op def train_and_eval_tree(X, y, seed) -\u0026gt; Tuple[DecisionTreeClassifier, float]: ... @op def eval_forest(trees:List[DecisionTreeClassifier], X, y) -\u0026gt; float: # get majority vote accuracy ... We can compose these functions in the obvious way to train a few trees and evaluate the resulting forest:\nwith storage.run(): # memoization context manager X, y = generate_data() trees = [] for seed in range(10): # can\u0026#39;t grow trees without seeds tree, acc = train_and_eval_tree(X, y, seed=seed) trees.append(tree) forest_acc = eval_forest(trees, X, y) Note that we are passing the list trees of memoized values into another memoized function. This incurs no storage duplication, because each element of the list is stored separately, and trees behaves like a list of pointers to storage locations, rather than a monolithic blob. This is one example of how mandala\u0026rsquo;s memoization \u0026ldquo;just works\u0026rdquo; with Python\u0026rsquo;s collections.\nThis compatibility lets you write algorithms that naturally operate on collections of objects (which do come up in practice, e.g. aggregators, model ensembling, clustering, chunking, \u0026hellip;) in the most straightforward way, while still having intermediate results cached (and even declaratively queriable). The video below shows this in action by incrementally evolving the workflow above with new logic and parameters:\nShow/hide video Your browser does not support HTML5 video. The video above illustrates\nmemoization as imperative query interface: you can get to any of the computed quantities just by executing this code, or parts of it, again. laziness: when run against a persistent storage, mandala avoids loading things from it when no new computations are being done (hence the objects with in_memory=False). control flow: laziness is designed to do the least work required by control flow - even though initially acc may be a lazy reference, when the if acc \u0026gt; 0.8: statement is reached, this causes a read from the storage. This applies more generally to collections: a lazy reference to a collection has no elements, but e.g. iterating over it automatically loads (lazy) references to its elements. Hierarchical memoization and incremental computing There\u0026rsquo;s still a major scalability problem - what if we had a workflow involving not tens but thousands of function calls? Stepping through each memoized call to get to a single final result (like we do with calls to train_and_eval_tree here) - even with laziness - could take a long time. Additional repetitive non-memoized logic along the way, which is necessary in more complex projects, can also slow things down needlessly.\nThe solution is simple: memoize problematic subprograms like that end-to-end by abstracting them as a single (parametrized) memoized function. After all, every time there is a large number of calls happening, they share some repetitive structure that can be abstracted away. This way you can take \u0026ldquo;shortcuts\u0026rdquo; in the memoization graph:\nShow/hide video Your browser does not support HTML5 video. The @superop decorator you see in the video is needed to indicate that a memoized function is itself a composition of memoized functions. A few of the nice things enabled by this:\nshortcutting memoization: the first time when you run the refactored workflow, execution goes inside the body of train_forest and loops over all the memoized calls to train_and_eval_tree, but the second time it has already memoized the call to train_forest end-to-end and jumps right to the return value. That\u0026rsquo;s why the second run is faster! natively incremental computing: calling train_forest with a higher value of n_trees does not do all its work from scratch: since it internally uses memoized functions, it is able to (lazily) skip over the memoized calls and do only the new work. Pattern-matching queries Using memoization as a query interface relies on you knowing the \u0026ldquo;initial conditions\u0026rdquo; from which to start stepping through the trail of memoized calls - but this is not always a good fit. As a complement to memoization\u0026rsquo;s imperative nature, you have a declarative query interface where you can replace unknown quantities with wildcard values. To illustrate, recall where we left things with our random forest example:\nIn [1]: with storage.run(): ...: X, y = generate_data() ...: for n_trees in (5, 10, 15, 20): ...: trees = train_forest(X, y, n_trees) ...: forest_acc = eval_forest(trees, X, y) If this project has been going for some weeks, you probably lost track of all the values of n_trees you ran this code with, and manually checking each one is too slow. The simplest thing would be to say something like n_trees = ? and let the system figure out the rest. That\u0026rsquo;s pretty much how it works:\nIn [2]: with storage.query() as q: # context manager for declarative queries ...: n_trees = Q() # a wildcard query variable ...: X, y = generate_data() # copy-paste computational code ...: trees = train_forest(X, y, n_trees) ...: forest_acc = eval_forest(trees, X, y) ...: df = q.get_table(n_trees.named(\u0026#39;n_trees\u0026#39;), ...: forest_acc.named(\u0026#39;forest_acc\u0026#39;)) ...: df Out[2]: n_trees forest_acc 0 10 0.99 1 20 0.97 2 15 0.96 3 5 0.94 What just happened? Behind the scenes, the code in a storage.query() block builds a computational graph that gets compiled to a SQL query over memoization tables. You pass variables to q.get_table(), and get back a table where each row is a matching of values to the variables that satisfies all the computational relationships in the graph. If a variable (like trees above) is not present in the columns, it is existentially quantified. This is the classical database concept of conjunctive queries.\nHere\u0026rsquo;s what the computational graph for the query above looks like:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?\u003e \u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e G 140261556989536 n_trees 140261556985072 X y n_trees train_forest(X, y, n_trees) output_0 140261556989536\u0026#45;\u0026gt;140261556985072:n_trees 140261556991216 forest_acc 140261556992176 X 140261556992176\u0026#45;\u0026gt;140261556985072:X 140261556982576 trees X y eval_forest(trees, X, y) output_0 140261556992176\u0026#45;\u0026gt;140261556982576:X 140261556991600 y 140261556991600\u0026#45;\u0026gt;140261556985072:y 140261556991600\u0026#45;\u0026gt;140261556982576:y 140261556985648 trees 140261556985648\u0026#45;\u0026gt;140261556982576:trees 140261556985072:output_0\u0026#45;\u0026gt;140261556985648 140261556982576:output_0\u0026#45;\u0026gt;140261556991216 140261556990112 generate_data() output_0 output_1 140261556990112:output_0\u0026#45;\u0026gt;140261556992176 140261556990112:output_1\u0026#45;\u0026gt;140261556991600 And here\u0026rsquo;s (simplified) SQL code the pattern-matching query compiles to, where __objs__ is a table of all the Python objects in the storage:\nSELECT n_trees, forest_acc FROM __objs__.obj as n_trees, __objs__.obj as X, __objs__.obj as y, __objs__.obj as trees, __objs__.obj as forest_acc, ... WHERE generate_data.output_0 = X and generate_data.output_1 = y and train_forest.X = X and train_forest.y = y and train_forest.n_trees = n_trees and train_forest.output_0 = trees and eval_forest.trees = trees and eval_forest.X = X and eval_forest.y = y and eval_forest.output_0 = forest_acc Queries compose with collections and memoization You can propagate more complex relationships through the declarative interface, such as ones between a collection and its elements (which allows you to query through operations that e.g. aggregate multiple objects). Furthermore, you can introduce pointwise constraints by directly passing objects recovered by memoization (or even scalars):\nIn [3]: with storage.run(): ...: X, y = generate_data() ...: tree, acc = train_and_eval_tree(X, y, seed=2) ...: with storage.query() as q: # contexts can be nested ...: trees = Q([tree, ...]) # matches a list containing `tree` ...: forest_acc = eval_forest(trees, X, y) ...: df = q.get_table(trees.named(\u0026#39;trees\u0026#39;), ...: forest_acc.named(\u0026#39;forest_acc\u0026#39;)) ...: df Out[3]: trees forest_acc 0 [DecisionTreeClassifier(max_depth=1, max_featu... 0.97 1 [DecisionTreeClassifier(max_depth=1, max_featu... 0.94 2 [DecisionTreeClassifier(max_depth=1, max_featu... 0.99 3 [DecisionTreeClassifier(max_depth=1, max_featu... 0.96 In effect, the above \u0026ldquo;hybrid\u0026rdquo; query is one way to \u0026ldquo;open up\u0026rdquo; the abstraction provided by the function train_forest, and query its internals.\nCo-evolve code and data So far, we treated memoized functions as unchanging, but that\u0026rsquo;s quite unrealistic. One of the thorniest problems of data management is maintaining a clear relationship between code and data in the face of changes in the internal logic of a project\u0026rsquo;s building blocks:\nif you ignore a change in logic, you generally end up with a mixture of results that look homogeneous to the system, but differ in the way they were generated. This makes comparisons between them more or less meaningless. if every change is automatically marked as a change in logic (looking at you, git), executions of semantically equivalent code will be confusingly scattered across versions. This makes comparisons between them difficult to program, and deters users from code quality improvements. An automatic and universal solution to this problem is fundamentally impossible. However, there are some easy-to-use opinionated tools to streamline the manual process of sorting changes into those that are irrelevant, compatible with past results, or incompatible (and thus require recomputation).\nExtending a function backward-compatibly A very common scenario in practice is that you have a function you\u0026rsquo;ve run a bunch of times, and then you get an idea for how it could do its work differently (e.g. use a new algorithm, or vary a previously hardcoded constant). Importantly, you want to keep the old results around, but somehow distinguish them from the new ones.\nIn mandala, you can do this by adding an argument with a default value, and it JustWorksâ„¢. A column is added to the current memoization table, with the default value applied retroactively to past calls. All the memoized calls without the new argument are still memoized. See below for how to expose a new hyperparameter to the pipeline, and add it to the declarative query as well:\nShow/hide video Your browser does not support HTML5 video. Function-level dependency tracking The harder problem is detecting when a function\u0026rsquo;s dependencies have changed, potentially rendering the memoized results out of sync with the new codebase. Since we\u0026rsquo;re basing everything on memoization, a purely git-style versioning model is too coarse-grained to work: a change in the logic of one memoized function should not invalidate the results of another memoized function that doesn\u0026rsquo;t depend on it!\nA dependency tracker for individual functions is needed, so that only the functions that are affected by a change can be recomputed. The problem is that a function\u0026rsquo;s dependencies are not just its source code and global variables, but also, recursively, the dependencies of all functions it calls. A perfect solution to this problem is difficult.\nCurrently, mandala opts for a conservative solution that may overestimate the set of dependencies for a particular call of a function, as well as the set of global variables a function depends on. When a dependency change is detected, you\u0026rsquo;re presented with a diff, the memoized functions that potentially depend on this change, and asked whether this change should be ignored (so old results are kept) or new versions of the memoized functions should be created (in which case old calls would be recomputed against the new codebase):\nShow/hide video Your browser does not support HTML5 video. This feature is still actively being designed, and there are many corner cases in which it (loudly) fails. However, when used properly, it gives you the best of both worlds: a tidy, comprehensible storage, and the ability to refactor code while retaining its connection to stored results.\nConclusion, and next up This was a rather brisk walk through mandala, but you\u0026rsquo;ve hopefully come away with a good sense of its general spirit and the sorts of simplicity it provides. Key features were not mentioned here, but are currently in the works:\nbatch execution: separate data management and execution concerns in cases when it\u0026rsquo;s more efficient to run function calls in batches, such as inference on large machine learning models. Code inside a with storage.batch() context is executed by a custom (batched) executor that you can implement, yet each call is individually memoized and queriable. parallelization: since mandala is function-based, it\u0026rsquo;s a great fit for function-based parallelization frameworks like dask. deletion interfaces: an imperative/declarative deletion interface, analogous to the imperative/declarative query interfaces. Just change with storage.run() to with storage.delete(). The plan for future blog posts is to focus on those, as well as on applications and the features described here in more depth.\nGetting involved We\u0026rsquo;re actively looking for more early adopters excited to try out mandala to manage their computational projects! While it is still a prototype of sorts, and there\u0026rsquo;s a lot to improve in terms of performance, it can handle a wide array of workloads quite well.\n","permalink":"https://amakelov.github.io/blog/pl/","summary":"Introduction They finally figured out how the Roman Empire actually fell, and it was this file: model_alpha=0.1_best_VERSION2_preprocess=True_final_FIXED.pkl. \u0026hellip;OK, we all agree this filename\u0026rsquo;s terrible - but why exactly? Its job is to tell the story of a computation. However, computations are more expressive, structured, and malleable than filenames. So, it\u0026rsquo;s no wonder you run into awkwardness, confusion and bugs when you try to turn one into the other. This impedance mismatch is at the root of the computational data management problem.","title":"Mandala: turning Python function calls into persistent, structured data"}]