<!doctype html><html lang=en dir=auto><head><link rel=stylesheet href=/style.css><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Mandala: turning Python function calls into persistent, structured data | Alex Makelov</title><meta name=keywords content><meta name=description content="Introduction They finally figured out how the Roman Empire actually fell, and it was this file: model_alpha=0.1_best_VERSION2_preprocess=True_final_FIXED.pkl. &mldr;OK, we all agree this filename&rsquo;s terrible - but why exactly? Its job is to tell the story of a computation. However, computations are more expressive, structured, and malleable than filenames. So, it&rsquo;s no wonder you run into awkwardness, confusion and bugs when you try to turn one into the other. This impedance mismatch is at the root of the computational data management problem."><meta name=author content><link rel=canonical href=https://amakelov.github.io/blog/pl/><link crossorigin=anonymous href=/assets/css/stylesheet.bccfefac377bc340f06c260aed1bddf49a4354816d7c570d6aac75a997986c95.css integrity="sha256-vM/vrDd7w0DwbCYK7Rvd9JpDVIFtfFcNaqx1qZeYbJU=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://amakelov.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://amakelov.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://amakelov.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://amakelov.github.io/apple-touch-icon.png><link rel=mask-icon href=https://amakelov.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Mandala: turning Python function calls into persistent, structured data"><meta property="og:description" content="Introduction They finally figured out how the Roman Empire actually fell, and it was this file: model_alpha=0.1_best_VERSION2_preprocess=True_final_FIXED.pkl. &mldr;OK, we all agree this filename&rsquo;s terrible - but why exactly? Its job is to tell the story of a computation. However, computations are more expressive, structured, and malleable than filenames. So, it&rsquo;s no wonder you run into awkwardness, confusion and bugs when you try to turn one into the other. This impedance mismatch is at the root of the computational data management problem."><meta property="og:type" content="article"><meta property="og:url" content="https://amakelov.github.io/blog/pl/"><meta property="og:image" content="https://amakelov.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="og:site_name" content="Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://amakelov.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Mandala: turning Python function calls into persistent, structured data"><meta name=twitter:description content="Introduction They finally figured out how the Roman Empire actually fell, and it was this file: model_alpha=0.1_best_VERSION2_preprocess=True_final_FIXED.pkl. &mldr;OK, we all agree this filename&rsquo;s terrible - but why exactly? Its job is to tell the story of a computation. However, computations are more expressive, structured, and malleable than filenames. So, it&rsquo;s no wonder you run into awkwardness, confusion and bugs when you try to turn one into the other. This impedance mismatch is at the root of the computational data management problem."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Blogs","item":"https://amakelov.github.io/blog/"},{"@type":"ListItem","position":3,"name":"Mandala: turning Python function calls into persistent, structured data","item":"https://amakelov.github.io/blog/pl/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Mandala: turning Python function calls into persistent, structured data","name":"Mandala: turning Python function calls into persistent, structured data","description":"Introduction They finally figured out how the Roman Empire actually fell, and it was this file: model_alpha=0.1_best_VERSION2_preprocess=True_final_FIXED.pkl. \u0026hellip;OK, we all agree this filename\u0026rsquo;s terrible - but why exactly? Its job is to tell the story of a computation. However, computations are more expressive, structured, and malleable than filenames. So, it\u0026rsquo;s no wonder you run into awkwardness, confusion and bugs when you try to turn one into the other. This impedance mismatch is at the root of the computational data management problem.","keywords":[],"articleBody":"Introduction They finally figured out how the Roman Empire actually fell, and it was this file: model_alpha=0.1_best_VERSION2_preprocess=True_final_FIXED.pkl. …OK, we all agree this filename’s terrible - but why exactly? Its job is to tell the story of a computation. However, computations are more expressive, structured, and malleable than filenames. So, it’s no wonder you run into awkwardness, confusion and bugs when you try to turn one into the other. This impedance mismatch is at the root of the computational data management problem.\nmandala: computations that save, load, and query themselves By contrast, the code producing the file’s contents is a far superior telling of its story:\nX, y = load_data() if preprocess: X = preprocess_data(X) model = train_v2(X, y, alpha=0.1) It makes it clear how logic and parameters combine to produce model. More subtly, implicit in the code are the invariant across executions computational relationships that we want to query (e.g., “find all the models trained with a particular dataset X”). If the code already fully describes what’s being computed, why can’t we just cut the (impedance-mismatched) middleman, and make the program its own storage interface?\nmandala is a tool that implements this idea. It turns function calls into composable, interlinked, queriable data that is automatically co-evolved with your codebase. By applying different semantics to this data, the same piece of ordinary Python code - possibly using control flow and collections - can be used to not only compute, but also save, load, query, delete, batch-compute, and combinations of those.\nCompared to existing frameworks addressing various parts of this problem, mandala bakes data management logic deeper into Python itself, and intentionally has few interfaces, concepts, and domain-specific features to speak of. Instead, it lets you do your work mostly through plain code. This direct approach makes it simpler to incrementally and interactively compute with and query experiments, especially in projects that compose many moving parts.\nOutline This blog post is a brief, somewhat programming-languages-themed tour of the unusual core features that work together to make this possible:\nmemoization aligned with core Python features to turn programs into imperative computation/storage/query interfaces and enable incremental computing; pattern-matching against code to turn programs into declarative query interfaces; interfaces to co-evolve code and data in a fine-grained way. All code examples used in this blog post are available in a notebook TODO.\nPython-integrated memoization While memoization is typically used as a mere cache, it can be much more. Imagine a complex piece of code where every function call has been memoized in a shared cache. Such code effectively becomes a map of the cache that you can traverse by re-execution, no matter how complex the logic. You can also incrementally build computations by just adding to this code: you get to the memoized part fast, and do the new work. This is especially useful in interactive environments like Jupyter notebooks.\nOrdinary memoization is not scalable enough for this, but it turns out that by baking memoization deeper into the language, you get a storage interface that is both practical and plain-Python.\nCollections, lazy reads, and control flow To illustrate, here’s a simple machine learning pipeline for a do-it-yourself random forest classifier:\n@op # memoization (\u0026 more) decorator def generate_data() -\u003e Tuple[ndarray, ndarray]: ... @op def train_and_eval_tree(X, y, seed) -\u003e Tuple[DecisionTreeClassifier, float]: ... @op def eval_forest(trees:List[DecisionTreeClassifier], X, y) -\u003e float: # get majority vote accuracy ... We can compose these functions in the obvious way to train a few trees and evaluate the resulting forest:\nwith storage.run(): # memoization context manager X, y = generate_data() trees = [] for seed in range(10): # can't grow trees without seeds tree, acc = train_and_eval_tree(X, y, seed=seed) trees.append(tree) forest_acc = eval_forest(trees, X, y) Note that we are passing the list trees of memoized values into another memoized function. This incurs no storage duplication, because each element of the list is stored separately, and trees behaves like a list of pointers to storage locations, rather than a monolithic blob. This is one example of how mandala’s memoization “just works” with Python’s collections.\nThis compatibility lets you write algorithms that naturally operate on collections of objects (which do come up in practice, e.g. aggregators, model ensembling, clustering, chunking, …) in the most straightforward way, while still having intermediate results cached (and even declaratively queriable). The video below shows this in action by incrementally evolving the workflow above with new logic and parameters:\nShow/hide video Your browser does not support HTML5 video. The video above illustrates\nmemoization as imperative query interface: you can get to any of the computed quantities just by executing this code, or parts of it, again. laziness: when run against a persistent storage, mandala avoids loading things from it when no new computations are being done (hence the objects with in_memory=False). control flow: laziness is designed to do the least work required by control flow - even though initially acc may be a lazy reference, when the if acc \u003e 0.8: statement is reached, this causes a read from the storage. This applies more generally to collections: a lazy reference to a collection has no elements, but e.g. iterating over it automatically loads (lazy) references to its elements. Hierarchical memoization and incremental computing There’s still a major scalability problem - what if we had a workflow involving not tens but thousands of function calls? Stepping through each memoized call to get to a single final result (like we do with calls to train_and_eval_tree here) - even with laziness - could take a long time. Additional repetitive non-memoized logic along the way, which is necessary in more complex projects, can also slow things down needlessly.\nThe solution is simple: memoize problematic subprograms like that end-to-end by abstracting them as a single (parametrized) memoized function. After all, every time there is a large number of calls happening, they share some repetitive structure that can be abstracted away. This way you can take “shortcuts” in the memoization graph:\nShow/hide video Your browser does not support HTML5 video. The @superop decorator you see in the video is needed to indicate that a memoized function is itself a composition of memoized functions. A few of the nice things enabled by this:\nshortcutting memoization: the first time when you run the refactored workflow, execution goes inside the body of train_forest and loops over all the memoized calls to train_and_eval_tree, but the second time it has already memoized the call to train_forest end-to-end and jumps right to the return value. That’s why the second run is faster! natively incremental computing: calling train_forest with a higher value of n_trees does not do all its work from scratch: since it internally uses memoized functions, it is able to (lazily) skip over the memoized calls and do only the new work. Pattern-matching queries Using memoization as a query interface relies on you knowing the “initial conditions” from which to start stepping through the trail of memoized calls - but this is not always a good fit. As a complement to memoization’s imperative nature, you have a declarative query interface where you can replace unknown quantities with wildcard values. To illustrate, recall where we left things with our random forest example:\nIn [1]: with storage.run(): ...: X, y = generate_data() ...: for n_trees in (5, 10, 15, 20): ...: trees = train_forest(X, y, n_trees) ...: forest_acc = eval_forest(trees, X, y) If this project has been going for some weeks, you probably lost track of all the values of n_trees you ran this code with, and manually checking each one is too slow. The simplest thing would be to say something like n_trees = ? and let the system figure out the rest. That’s pretty much how it works:\nIn [2]: with storage.query() as q: # context manager for declarative queries ...: n_trees = Q() # a wildcard query variable ...: X, y = generate_data() # copy-paste computational code ...: trees = train_forest(X, y, n_trees) ...: forest_acc = eval_forest(trees, X, y) ...: df = q.get_table(n_trees.named('n_trees'), ...: forest_acc.named('forest_acc')) ...: df Out[2]: n_trees forest_acc 0 10 0.99 1 20 0.97 2 15 0.96 3 5 0.94 What just happened? Behind the scenes, the code in a storage.query() block builds a computational graph that gets compiled to a SQL query over memoization tables. You pass variables to q.get_table(), and get back a table where each row is a matching of values to the variables that satisfies all the computational relationships in the graph. If a variable (like trees above) is not present in the columns, it is existentially quantified. This is the classical database concept of conjunctive queries.\nHere’s what the computational graph for the query above looks like:\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?\u003e \u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e G 140261556989536 n_trees 140261556985072 X y n_trees train_forest(X, y, n_trees) output_0 140261556989536-\u003e140261556985072:n_trees 140261556991216 forest_acc 140261556992176 X 140261556992176-\u003e140261556985072:X 140261556982576 trees X y eval_forest(trees, X, y) output_0 140261556992176-\u003e140261556982576:X 140261556991600 y 140261556991600-\u003e140261556985072:y 140261556991600-\u003e140261556982576:y 140261556985648 trees 140261556985648-\u003e140261556982576:trees 140261556985072:output_0-\u003e140261556985648 140261556982576:output_0-\u003e140261556991216 140261556990112 generate_data() output_0 output_1 140261556990112:output_0-\u003e140261556992176 140261556990112:output_1-\u003e140261556991600 And here’s (simplified) SQL code the pattern-matching query compiles to, where __objs__ is a table of all the Python objects in the storage:\nSELECT n_trees, forest_acc FROM __objs__.obj as n_trees, __objs__.obj as X, __objs__.obj as y, __objs__.obj as trees, __objs__.obj as forest_acc, ... WHERE generate_data.output_0 = X and generate_data.output_1 = y and train_forest.X = X and train_forest.y = y and train_forest.n_trees = n_trees and train_forest.output_0 = trees and eval_forest.trees = trees and eval_forest.X = X and eval_forest.y = y and eval_forest.output_0 = forest_acc Queries compose with collections and memoization You can propagate more complex relationships through the declarative interface, such as ones between a collection and its elements (which allows you to query through operations that e.g. aggregate multiple objects). Furthermore, you can introduce pointwise constraints by directly passing objects recovered by memoization (or even scalars):\nIn [3]: with storage.run(): ...: X, y = generate_data() ...: tree, acc = train_and_eval_tree(X, y, seed=2) ...: with storage.query() as q: # contexts can be nested ...: trees = Q([tree, ...]) # matches a list containing `tree` ...: forest_acc = eval_forest(trees, X, y) ...: df = q.get_table(trees.named('trees'), ...: forest_acc.named('forest_acc')) ...: df Out[3]: trees forest_acc 0 [DecisionTreeClassifier(max_depth=1, max_featu... 0.97 1 [DecisionTreeClassifier(max_depth=1, max_featu... 0.94 2 [DecisionTreeClassifier(max_depth=1, max_featu... 0.99 3 [DecisionTreeClassifier(max_depth=1, max_featu... 0.96 In effect, the above “hybrid” query is one way to “open up” the abstraction provided by the function train_forest, and query its internals.\nCo-evolve code and data So far, we treated memoized functions as unchanging, but that’s quite unrealistic. One of the thorniest problems of data management is maintaining a clear relationship between code and data in the face of changes in the internal logic of a project’s building blocks:\nif you ignore a change in logic, you generally end up with a mixture of results that look homogeneous to the system, but differ in the way they were generated. This makes comparisons between them more or less meaningless. if every change is automatically marked as a change in logic (looking at you, git), executions of semantically equivalent code will be confusingly scattered across versions. This makes comparisons between them difficult to program, and deters users from code quality improvements. An automatic and universal solution to this problem is fundamentally impossible. However, there are some easy-to-use opinionated tools to streamline the manual process of sorting changes into those that are irrelevant, compatible with past results, or incompatible (and thus require recomputation).\nExtending a function backward-compatibly A very common scenario in practice is that you have a function you’ve run a bunch of times, and then you get an idea for how it could do its work differently (e.g. use a new algorithm, or vary a previously hardcoded constant). Importantly, you want to keep the old results around, but somehow distinguish them from the new ones.\nIn mandala, you can do this by adding an argument with a default value, and it JustWorks™. A column is added to the current memoization table, with the default value applied retroactively to past calls. All the memoized calls without the new argument are still memoized. See below for how to expose a new hyperparameter to the pipeline, and add it to the declarative query as well:\nShow/hide video Your browser does not support HTML5 video. Function-level dependency tracking The harder problem is detecting when a function’s dependencies have changed, potentially rendering the memoized results out of sync with the new codebase. Since we’re basing everything on memoization, a purely git-style versioning model is too coarse-grained to work: a change in the logic of one memoized function should not invalidate the results of another memoized function that doesn’t depend on it!\nA dependency tracker for individual functions is needed, so that only the functions that are affected by a change can be recomputed. The problem is that a function’s dependencies are not just its source code and global variables, but also, recursively, the dependencies of all functions it calls. A perfect solution to this problem is difficult.\nCurrently, mandala opts for a conservative solution that may overestimate the set of dependencies for a particular call of a function, as well as the set of global variables a function depends on. When a dependency change is detected, you’re presented with a diff, the memoized functions that potentially depend on this change, and asked whether this change should be ignored (so old results are kept) or new versions of the memoized functions should be created (in which case old calls would be recomputed against the new codebase):\nShow/hide video Your browser does not support HTML5 video. This feature is still actively being designed, and there are many corner cases in which it (loudly) fails. However, when used properly, it gives you the best of both worlds: a tidy, comprehensible storage, and the ability to refactor code while retaining its connection to stored results.\nConclusion, and next up This was a rather brisk walk through mandala, but you’ve hopefully come away with a good sense of its general spirit and the sorts of simplicity it provides. Key features were not mentioned here, but are currently in the works:\nbatch execution: separate data management and execution concerns in cases when it’s more efficient to run function calls in batches, such as inference on large machine learning models. Code inside a with storage.batch() context is executed by a custom (batched) executor that you can implement, yet each call is individually memoized and queriable. parallelization: since mandala is function-based, it’s a great fit for function-based parallelization frameworks like dask. deletion interfaces: an imperative/declarative deletion interface, analogous to the imperative/declarative query interfaces. Just change with storage.run() to with storage.delete(). The plan for future blog posts is to focus on those, as well as on applications and the features described here in more depth.\nGetting involved We’re actively looking for more early adopters excited to try out mandala to manage their computational projects! While it is still a prototype of sorts, and there’s a lot to improve in terms of performance, it can handle a wide array of workloads quite well.\n","wordCount":"2492","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://amakelov.github.io/blog/pl/"},"publisher":{"@type":"Organization","name":"Alex Makelov","logo":{"@type":"ImageObject","url":"https://amakelov.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://amakelov.github.io accesskey=h title="Alex Makelov (Alt + H)">Alex Makelov</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Mandala: turning Python function calls into persistent, structured data</h1><div class=post-meta></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a><ul><li><a href=#mandala-computations-that-save-load-and-query-themselves><code>mandala</code>: computations that save, load, and query themselves</a></li><li><a href=#outline>Outline</a></li></ul></li><li><a href=#python-integrated-memoization>Python-integrated memoization</a><ul><li><a href=#collections-lazy-reads-and-control-flow>Collections, lazy reads, and control flow</a></li><li><a href=#hierarchical-memoization-and-incremental-computing>Hierarchical memoization and incremental computing</a></li></ul></li><li><a href=#pattern-matching-queries>Pattern-matching queries</a><ul><li><a href=#what-just-happened>What just happened?</a></li><li><a href=#queries-compose-with-collections-and-memoization>Queries compose with collections and memoization</a></li></ul></li><li><a href=#co-evolve-code-and-data>Co-evolve code and data</a><ul><li><a href=#extending-a-function-backward-compatibly>Extending a function backward-compatibly</a></li><li><a href=#function-level-dependency-tracking>Function-level dependency tracking</a></li></ul></li><li><a href=#conclusion-and-next-up>Conclusion, and next up</a><ul><li><a href=#getting-involved>Getting involved</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>They finally figured out how the Roman Empire <em>actually</em> fell, and it was
this file: <code>model_alpha=0.1_best_VERSION2_preprocess=True_final_FIXED.pkl</code>. &mldr;OK,
we all agree this filename&rsquo;s terrible - but why exactly? Its job is to tell the
story of a computation. However, computations are more expressive, structured,
and malleable than filenames. So, it&rsquo;s no wonder you run into awkwardness,
confusion and bugs when you try to turn one into the other. This impedance
mismatch is at the root of the <strong>computational data management problem</strong>.</p><h3 id=mandala-computations-that-save-load-and-query-themselves><code>mandala</code>: computations that save, load, and query themselves<a hidden class=anchor aria-hidden=true href=#mandala-computations-that-save-load-and-query-themselves>#</a></h3><p>By contrast, the code producing the file&rsquo;s contents is a far superior telling of
its story:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>load_data</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>preprocess</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>X</span> <span class=o>=</span> <span class=n>preprocess_data</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>train_v2</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>
</span></span></code></pre></div><p>It makes it clear how logic and parameters combine to produce <code>model</code>.
More subtly, implicit in the code are the invariant across executions
computational relationships that we want to query (e.g., &ldquo;find all the models
trained with a particular dataset <code>X</code>&rdquo;). If the code already fully describes
what&rsquo;s being computed, why can&rsquo;t we just cut the (impedance-mismatched)
middleman, and <strong>make the program its own storage interface</strong>?</p><p><a href=https://github.com/amakelov/mandala><code>mandala</code></a> is a tool that implements this
idea. It turns function calls into composable, interlinked, queriable data that
is automatically co-evolved with your codebase. By applying different semantics
to this data, the same piece of ordinary Python code - possibly using control flow
and collections - can be used to not only compute, but also save, load, query,
delete, batch-compute, and combinations of those.</p><p><a href=https://wandb.ai/site>Compared</a> <a href=https://github.com/IDSIA/sacred>to</a>
<a href=https://mlflow.org/>existing</a> <a href=https://github.com/iterative/dvc>frameworks</a>
addressing various parts of this problem,
<code>mandala</code> bakes data management logic deeper into Python itself, and
intentionally has few interfaces, concepts, and domain-specific features to
speak of. Instead, it lets you do your work mostly through plain code. This
direct approach makes it simpler to incrementally and interactively
compute with and query experiments, especially in projects that compose many
moving parts.</p><h3 id=outline>Outline<a hidden class=anchor aria-hidden=true href=#outline>#</a></h3><p>This blog post is a brief, somewhat programming-languages-themed tour of
the unusual core features that work together to make this possible:</p><ul><li><a href=#python-integrated-memoization>memoization</a> aligned with core Python
features to turn programs into <em>imperative</em> computation/storage/query interfaces
and enable incremental computing;</li><li><a href=#pattern-matching-queries>pattern-matching against code</a> to turn programs
into <em>declarative</em> query interfaces;</li><li>interfaces to <a href=#co-evolve-code-and-data>co-evolve code and data</a> in a
fine-grained way.</li></ul><p>All code examples used in this blog post are available in a notebook TODO.</p><h2 id=python-integrated-memoization>Python-integrated memoization<a hidden class=anchor aria-hidden=true href=#python-integrated-memoization>#</a></h2><p>While <a href=https://en.wikipedia.org/wiki/Memoization>memoization</a> is typically used
as a mere cache, it can be much more. Imagine a complex piece of code where
<em>every</em> function call has been memoized in a shared cache. Such code effectively
becomes a map of the cache that you can traverse by re-execution, no matter how
complex the logic. You can also incrementally build computations by just adding
to this code: you get to the memoized part fast, and do the new work. This is
especially useful in interactive environments like <a href=https://jupyter.org/>Jupyter notebooks</a>.</p><p>Ordinary memoization is not scalable enough for this, but it turns out that by
baking memoization deeper into the language, you get a storage
interface that is both practical and plain-Python.</p><h3 id=collections-lazy-reads-and-control-flow>Collections, lazy reads, and control flow<a hidden class=anchor aria-hidden=true href=#collections-lazy-reads-and-control-flow>#</a></h3><p>To illustrate, here&rsquo;s a simple machine learning pipeline for a do-it-yourself
random forest classifier:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nd>@op</span> <span class=c1># memoization (&amp; more) decorator</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>generate_data</span><span class=p>()</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>ndarray</span><span class=p>,</span> <span class=n>ndarray</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@op</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>train_and_eval_tree</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>seed</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>DecisionTreeClassifier</span><span class=p>,</span> <span class=nb>float</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=nd>@op</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>eval_forest</span><span class=p>(</span><span class=n>trees</span><span class=p>:</span><span class=n>List</span><span class=p>[</span><span class=n>DecisionTreeClassifier</span><span class=p>],</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>float</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># get majority vote accuracy</span>
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span></code></pre></div><p>We can compose these functions in the obvious way to train a few trees and
evaluate the resulting forest:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>with</span> <span class=n>storage</span><span class=o>.</span><span class=n>run</span><span class=p>():</span> <span class=c1># memoization context manager</span>
</span></span><span class=line><span class=cl>    <span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>generate_data</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>trees</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>seed</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>10</span><span class=p>):</span> <span class=c1># can&#39;t grow trees without seeds</span>
</span></span><span class=line><span class=cl>        <span class=n>tree</span><span class=p>,</span> <span class=n>acc</span> <span class=o>=</span> <span class=n>train_and_eval_tree</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>seed</span><span class=o>=</span><span class=n>seed</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>trees</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>tree</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>forest_acc</span> <span class=o>=</span> <span class=n>eval_forest</span><span class=p>(</span><span class=n>trees</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span></code></pre></div><p>Note that we are passing the list <code>trees</code> of memoized values into another
memoized function. This incurs no storage duplication, because each element of
the list is stored separately, and <code>trees</code> behaves like a list of pointers to
storage locations, rather than a monolithic blob. This is one example of how
<code>mandala</code>&rsquo;s memoization &ldquo;just works&rdquo; with Python&rsquo;s collections.</p><p>This compatibility lets you write algorithms that naturally operate on
collections of objects (which <em>do</em> come up in practice, e.g. aggregators, model
ensembling, clustering, chunking, &mldr;) in the most straightforward way, while
still having intermediate results cached (and even <a href=#pattern-matching-queries>declaratively
queriable</a>). The video below shows this in action by
incrementally evolving the workflow above with new logic and parameters:</p><p><details open><summary markdown=span>Show/hide video</summary>
<video width=100% controls>
<source src=../../videos/interactive_memoization.mp4 type=video/mp4><source src=../../videos/interactive_memoization.webm type=video/webm>Your browser does not support HTML5 video.</video></details></p><p>The video above illustrates</p><ul><li><strong>memoization as imperative query interface</strong>: you can get to any of the
computed quantities just by executing this code, or parts of it, again.</li><li><strong>laziness</strong>: when run against a persistent storage, <code>mandala</code> avoids
loading things from it when no new computations are being done (hence the
objects with <code>in_memory=False</code>).</li><li><strong>control flow</strong>: laziness is designed to do the least work required by
control flow - even though initially <code>acc</code> may be a lazy reference, when the <code>if acc > 0.8:</code> statement is reached, this causes a read from the storage. This
applies more generally to collections: a lazy reference to a collection has no
elements, but e.g. iterating over it automatically loads (lazy) references to its
elements.</li></ul><h3 id=hierarchical-memoization-and-incremental-computing>Hierarchical memoization and incremental computing<a hidden class=anchor aria-hidden=true href=#hierarchical-memoization-and-incremental-computing>#</a></h3><p>There&rsquo;s still a major scalability problem - what if we had a workflow
involving not tens but thousands of function calls? Stepping through each
memoized call to get to a single final result (like we do with calls to
<code>train_and_eval_tree</code> here) - even with laziness - could take a long time.
Additional repetitive non-memoized logic along the way, which is necessary in
more complex projects, can also slow things down needlessly.</p><p>The solution is simple: memoize problematic subprograms like that end-to-end by
abstracting them as a single (parametrized) memoized function. After all, every
time there is a large number of calls happening, they share some repetitive
structure that can be abstracted away. This way you can take &ldquo;shortcuts&rdquo; in the
memoization graph:</p><p><details open><summary markdown=span>Show/hide video</summary>
<video width=100% controls>
<source src=../../videos/sup.mp4 type=video/mp4><source src=../../videos/sup.webm type=video/webm>Your browser does not support HTML5 video.</video></details></p><p>The <code>@superop</code> decorator you see in the video is needed to indicate that a
memoized function is itself a composition of memoized functions. A few of the
nice things enabled by this:</p><ul><li><strong>shortcutting memoization</strong>: the first time when you run the refactored
workflow, execution goes inside the body of <code>train_forest</code> and loops over all
the memoized calls to <code>train_and_eval_tree</code>, but the second time it has already
memoized the call to <code>train_forest</code> end-to-end and jumps right to the return
value. That&rsquo;s why the second run is faster!</li><li><strong>natively incremental computing</strong>: calling <code>train_forest</code> with a higher value of
<code>n_trees</code> does not do all its work from scratch: since it internally uses
memoized functions, it is able to (lazily) skip over the memoized calls and do
only the new work.</li></ul><h2 id=pattern-matching-queries>Pattern-matching queries<a hidden class=anchor aria-hidden=true href=#pattern-matching-queries>#</a></h2><p>Using memoization as a query interface relies on you knowing the &ldquo;initial
conditions&rdquo; from which to start stepping through the trail of memoized calls -
but this is not always a good fit. As a complement to memoization&rsquo;s imperative
nature, you have a <em>declarative</em> query interface where you can replace unknown quantities with
wildcard values. To illustrate, recall <a href=#hierarchical-memoization-and-incremental-computing>where we left
things</a> with our random
forest example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>In</span> <span class=p>[</span><span class=mi>1</span><span class=p>]:</span> <span class=k>with</span> <span class=n>storage</span><span class=o>.</span><span class=n>run</span><span class=p>():</span>
</span></span><span class=line><span class=cl>   <span class=o>...</span><span class=p>:</span>     <span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>generate_data</span><span class=p>()</span>
</span></span><span class=line><span class=cl>   <span class=o>...</span><span class=p>:</span>     <span class=k>for</span> <span class=n>n_trees</span> <span class=ow>in</span> <span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>15</span><span class=p>,</span> <span class=mi>20</span><span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=o>...</span><span class=p>:</span>         <span class=n>trees</span> <span class=o>=</span> <span class=n>train_forest</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>n_trees</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=o>...</span><span class=p>:</span>         <span class=n>forest_acc</span> <span class=o>=</span> <span class=n>eval_forest</span><span class=p>(</span><span class=n>trees</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span></code></pre></div><p>If this project has been going for some weeks, you probably lost track of all
the values of <code>n_trees</code> you ran this code with, and manually checking each one
is too slow. The simplest thing would be to say something like <code>n_trees = ?</code> and
let the system figure out the rest. That&rsquo;s pretty much how it works:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>In</span> <span class=p>[</span><span class=mi>2</span><span class=p>]:</span> <span class=k>with</span> <span class=n>storage</span><span class=o>.</span><span class=n>query</span><span class=p>()</span> <span class=k>as</span> <span class=n>q</span><span class=p>:</span> <span class=c1># context manager for declarative queries</span>
</span></span><span class=line><span class=cl>   <span class=o>...</span><span class=p>:</span>     <span class=n>n_trees</span> <span class=o>=</span> <span class=n>Q</span><span class=p>()</span> <span class=c1># a wildcard query variable</span>
</span></span><span class=line><span class=cl>   <span class=o>...</span><span class=p>:</span>     <span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>generate_data</span><span class=p>()</span> <span class=c1># copy-paste computational code</span>
</span></span><span class=line><span class=cl>   <span class=o>...</span><span class=p>:</span>     <span class=n>trees</span> <span class=o>=</span> <span class=n>train_forest</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>n_trees</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=o>...</span><span class=p>:</span>     <span class=n>forest_acc</span> <span class=o>=</span> <span class=n>eval_forest</span><span class=p>(</span><span class=n>trees</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=o>...</span><span class=p>:</span>     <span class=n>df</span> <span class=o>=</span> <span class=n>q</span><span class=o>.</span><span class=n>get_table</span><span class=p>(</span><span class=n>n_trees</span><span class=o>.</span><span class=n>named</span><span class=p>(</span><span class=s1>&#39;n_trees&#39;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>   <span class=o>...</span><span class=p>:</span>                      <span class=n>forest_acc</span><span class=o>.</span><span class=n>named</span><span class=p>(</span><span class=s1>&#39;forest_acc&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>   <span class=o>...</span><span class=p>:</span> <span class=n>df</span>
</span></span><span class=line><span class=cl><span class=n>Out</span><span class=p>[</span><span class=mi>2</span><span class=p>]:</span> 
</span></span><span class=line><span class=cl>   <span class=n>n_trees</span>  <span class=n>forest_acc</span>
</span></span><span class=line><span class=cl><span class=mi>0</span>       <span class=mi>10</span>        <span class=mf>0.99</span>
</span></span><span class=line><span class=cl><span class=mi>1</span>       <span class=mi>20</span>        <span class=mf>0.97</span>
</span></span><span class=line><span class=cl><span class=mi>2</span>       <span class=mi>15</span>        <span class=mf>0.96</span>
</span></span><span class=line><span class=cl><span class=mi>3</span>        <span class=mi>5</span>        <span class=mf>0.94</span>
</span></span></code></pre></div><h3 id=what-just-happened>What just happened?<a hidden class=anchor aria-hidden=true href=#what-just-happened>#</a></h3><p>Behind the scenes, the code in a <code>storage.query()</code> block builds a computational
graph that gets compiled to a SQL query over memoization tables. You pass
variables to <code>q.get_table()</code>, and get back a table where each row is a matching
of values to the variables that satisfies <em>all</em> the computational relationships
in the graph. If a variable (like <code>trees</code> above) is not present in the columns,
it is existentially quantified. This is the classical database concept of
<a href=https://en.wikipedia.org/wiki/Conjunctive_query>conjunctive queries</a>.</p><p>Here&rsquo;s what the computational graph for the query above looks like:</p><figure align=center class=align-center><!doctype html><svg width="286pt" height="375pt" viewBox="0 0 285.64 375" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 371)"><title>G</title><polygon fill="#fff" stroke="transparent" points="-4,4 -4,-371 281.64,-371 281.64,4 -4,4"/><g id="node1" class="node"><title>140261556989536</title><polygon fill="#cb4b16" stroke="transparent" points="183.5,-279.5 183.5,-296.5 231.5,-296.5 231.5,-279.5 183.5,-279.5"/><polygon fill="none" stroke="#002b36" points="183.5,-279.5 183.5,-296.5 231.5,-296.5 231.5,-279.5 183.5,-279.5"/><text text-anchor="start" x="186.5" y="-286.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#fdf6e3">n_trees</text></g><g id="node6" class="node"><title>140261556985072</title><polygon fill="#fdf6e3" stroke="transparent" points="111.5,-226.5 111.5,-243.5 163.5,-243.5 163.5,-226.5 111.5,-226.5"/><polygon fill="none" stroke="#002b36" points="111.5,-226.5 111.5,-243.5 163.5,-243.5 163.5,-226.5 111.5,-226.5"/><text text-anchor="start" x="133.5" y="-233.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#002b36">X</text><polygon fill="#fdf6e3" stroke="transparent" points="163.5,-226.5 163.5,-243.5 195.5,-243.5 195.5,-226.5 163.5,-226.5"/><polygon fill="none" stroke="#002b36" points="163.5,-226.5 163.5,-243.5 195.5,-243.5 195.5,-226.5 163.5,-226.5"/><text text-anchor="start" x="176" y="-233.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#002b36">y</text><polygon fill="#fdf6e3" stroke="transparent" points="195.5,-226.5 195.5,-243.5 262.5,-243.5 262.5,-226.5 195.5,-226.5"/><polygon fill="none" stroke="#002b36" points="195.5,-226.5 195.5,-243.5 262.5,-243.5 262.5,-226.5 195.5,-226.5"/><text text-anchor="start" x="208" y="-233.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#002b36">n_trees</text><polygon fill="#268bd2" stroke="transparent" points="111.5,-209.5 111.5,-226.5 262.5,-226.5 262.5,-209.5 111.5,-209.5"/><polygon fill="none" stroke="#002b36" points="111.5,-209.5 111.5,-226.5 262.5,-226.5 262.5,-209.5 111.5,-209.5"/><text text-anchor="start" x="114.5" y="-216.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#fdf6e3">train_forest(X, y, n_trees)</text><polygon fill="#fdf6e3" stroke="transparent" points="111.5,-192.5 111.5,-209.5 262.5,-209.5 262.5,-192.5 111.5,-192.5"/><polygon fill="none" stroke="#002b36" points="111.5,-192.5 111.5,-209.5 262.5,-209.5 262.5,-192.5 111.5,-192.5"/><text text-anchor="start" x="162.5" y="-199.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#002b36">output_0</text></g><g id="edge3" class="edge"><title>140261556989536->140261556985072:n_trees</title><path fill="none" stroke="#002b36" d="M214.22-279.76C219.07-273.56 225.24-264.25 228.02-254.21"/><polygon fill="#002b36" stroke="#002b36" points="231.53,-254.4 229.5,-244 224.6,-253.4 231.53,-254.4"/></g><g id="node2" class="node"><title>140261556991216</title><polygon fill="#cb4b16" stroke="transparent" points="37.5,0.5 37.5,-16.5 101.5,-16.5 101.5,0.5 37.5,0.5"/><polygon fill="none" stroke="#002b36" points="37.5,0.5 37.5,-16.5 101.5,-16.5 101.5,0.5 37.5,0.5"/><text text-anchor="start" x="40.5" y="-6.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#fdf6e3">forest_acc</text></g><g id="node3" class="node"><title>140261556992176</title><polygon fill="#cb4b16" stroke="transparent" points="130.5,-279.5 130.5,-296.5 144.5,-296.5 144.5,-279.5 130.5,-279.5"/><polygon fill="none" stroke="#002b36" points="130.5,-279.5 130.5,-296.5 144.5,-296.5 144.5,-279.5 130.5,-279.5"/><text text-anchor="start" x="133.5" y="-286.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#fdf6e3">X</text></g><g id="edge1" class="edge"><title>140261556992176->140261556985072:X</title><path fill="none" stroke="#002b36" d="M137.5-279.8c0 6.38999999999999.0 15.94.0 25.53"/><polygon fill="#002b36" stroke="#002b36" points="141,-254 137.5,-244 134,-254 141,-254"/></g><g id="node7" class="node"><title>140261556982576</title><polygon fill="#fdf6e3" stroke="transparent" points="0.5,-86.5 0.5,-103.5 73.5,-103.5 73.5,-86.5 0.5,-86.5"/><polygon fill="none" stroke="#002b36" points="0.5,-86.5 0.5,-103.5 73.5,-103.5 73.5,-86.5 0.5,-86.5"/><text text-anchor="start" x="22" y="-93.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#002b36">trees</text><polygon fill="#fdf6e3" stroke="transparent" points="73.5,-86.5 73.5,-103.5 105.5,-103.5 105.5,-86.5 73.5,-86.5"/><polygon fill="none" stroke="#002b36" points="73.5,-86.5 73.5,-103.5 105.5,-103.5 105.5,-86.5 73.5,-86.5"/><text text-anchor="start" x="85.5" y="-93.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#002b36">X</text><polygon fill="#fdf6e3" stroke="transparent" points="105.5,-86.5 105.5,-103.5 137.5,-103.5 137.5,-86.5 105.5,-86.5"/><polygon fill="none" stroke="#002b36" points="105.5,-86.5 105.5,-103.5 137.5,-103.5 137.5,-86.5 105.5,-86.5"/><text text-anchor="start" x="118" y="-93.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#002b36">y</text><polygon fill="#268bd2" stroke="transparent" points="0.5,-69.5 0.5,-86.5 137.5,-86.5 137.5,-69.5 0.5,-69.5"/><polygon fill="none" stroke="#002b36" points="0.5,-69.5 0.5,-86.5 137.5,-86.5 137.5,-69.5 0.5,-69.5"/><text text-anchor="start" x="3.5" y="-76.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#fdf6e3">eval_forest(trees, X, y)</text><polygon fill="#fdf6e3" stroke="transparent" points="0.5,-52.5 0.5,-69.5 137.5,-69.5 137.5,-52.5 0.5,-52.5"/><polygon fill="none" stroke="#002b36" points="0.5,-52.5 0.5,-69.5 137.5,-69.5 137.5,-52.5 0.5,-52.5"/><text text-anchor="start" x="44.5" y="-59.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#002b36">output_0</text></g><g id="edge6" class="edge"><title>140261556992176->140261556982576:X</title><path fill="none" stroke="#002b36" d="M130.48-281.56C121.94-273.88 107.89-259.6 101.5-244c-22.31 54.52-13.21 75.19-12.1 129.78"/><polygon fill="#002b36" stroke="#002b36" points="92.9,-114.04 89.5,-104 85.9,-113.96 92.9,-114.04"/></g><g id="node4" class="node"><title>140261556991600</title><polygon fill="#cb4b16" stroke="transparent" points="250.5,-279.5 250.5,-296.5 263.5,-296.5 263.5,-279.5 250.5,-279.5"/><polygon fill="none" stroke="#002b36" points="250.5,-279.5 250.5,-296.5 263.5,-296.5 263.5,-279.5 250.5,-279.5"/><text text-anchor="start" x="253.5" y="-286.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#fdf6e3">y</text></g><g id="edge2" class="edge"><title>140261556991600->140261556985072:y</title><path fill="none" stroke="#002b36" d="M249.94-284.18C235.82-277.01 202.65-259.95 187.41-250.35"/><polygon fill="#002b36" stroke="#002b36" points="189.49,-247.53 179.5,-244 185.11,-252.99 189.49,-247.53"/></g><g id="edge7" class="edge"><title>140261556991600->140261556982576:y</title><path fill="none" stroke="#002b36" d="M260.62-279.92C269.09-263.23 286.44-222.49 271.5-193c-31.58 62.31-57.2 94.42-123 97.29"/><polygon fill="#002b36" stroke="#002b36" points="148.57,-92.21 138.5,-95.5 148.42,-99.21 148.57,-92.21"/></g><g id="node5" class="node"><title>140261556985648</title><polygon fill="#cb4b16" stroke="transparent" points="18.5,-139.5 18.5,-156.5 54.5,-156.5 54.5,-139.5 18.5,-139.5"/><polygon fill="none" stroke="#002b36" points="18.5,-139.5 18.5,-156.5 54.5,-156.5 54.5,-139.5 18.5,-139.5"/><text text-anchor="start" x="21.5" y="-146.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#fdf6e3">trees</text></g><g id="edge5" class="edge"><title>140261556985648->140261556982576:trees</title><path fill="none" stroke="#002b36" d="M36.5-139.8c0 6.39000000000002.0 15.94.0 25.53"/><polygon fill="#002b36" stroke="#002b36" points="40,-114 36.5,-104 33,-114 40,-114"/></g><g id="edge4" class="edge"><title>140261556985072:output_0->140261556985648</title><path fill="none" stroke="#002b36" d="M110.5-200.5c-25.3.0-48.24 20.47-61.66 35.47"/><polygon fill="#002b36" stroke="#002b36" points="51.21,-162.42 42.09,-157.03 45.86,-166.93 51.21,-162.42"/></g><g id="edge8" class="edge"><title>140261556982576:output_0->140261556991216</title><path fill="none" stroke="#002b36" d="M69.5-53c0 8.42.0 17.76.0 25.62"/><polygon fill="#002b36" stroke="#002b36" points="73,-27.2 69.5,-17.2 66,-27.2 73,-27.2"/></g><g id="node8" class="node"><title>140261556990112</title><polygon fill="#268bd2" stroke="transparent" points="131.5,-350 131.5,-367 241.5,-367 241.5,-350 131.5,-350"/><polygon fill="none" stroke="#002b36" points="131.5,-350 131.5,-367 241.5,-367 241.5,-350 131.5,-350"/><text text-anchor="start" x="141" y="-357" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#fdf6e3">generate_data()</text><polygon fill="#fdf6e3" stroke="transparent" points="131.5,-333 131.5,-350 186.5,-350 186.5,-333 131.5,-333"/><polygon fill="none" stroke="#002b36" points="131.5,-333 131.5,-350 186.5,-350 186.5,-333 131.5,-333"/><text text-anchor="start" x="134.5" y="-340" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#002b36">output_0</text><polygon fill="#fdf6e3" stroke="transparent" points="186.5,-333 186.5,-350 241.5,-350 241.5,-333 186.5,-333"/><polygon fill="none" stroke="#002b36" points="186.5,-333 186.5,-350 241.5,-350 241.5,-333 186.5,-333"/><text text-anchor="start" x="189.5" y="-340" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#002b36">output_1</text></g><g id="edge9" class="edge"><title>140261556990112:output_0->140261556992176</title><path fill="none" stroke="#002b36" d="M158.5-333C158.5-323.34 154.25-313.57 149.57-305.74"/><polygon fill="#002b36" stroke="#002b36" points="152.39,-303.65 143.91,-297.29 146.57,-307.55 152.39,-303.65"/></g><g id="edge10" class="edge"><title>140261556990112:output_1->140261556991600</title><path fill="none" stroke="#002b36" d="M242.5-341c14.32.0 16.45 18.78 15.89 33.66"/><polygon fill="#002b36" stroke="#002b36" points="261.86,-306.8 257.6,-297.1 254.88,-307.34 261.86,-306.8"/></g></g></svg></figure><p>And here&rsquo;s (simplified) SQL code the pattern-matching query compiles to, where
<code>__objs__</code> is a table of all the Python objects in the storage:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=cl><span class=k>SELECT</span><span class=w> </span><span class=n>n_trees</span><span class=p>,</span><span class=w> </span><span class=n>forest_acc</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>FROM</span><span class=w> 
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>__objs__</span><span class=p>.</span><span class=n>obj</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=n>n_trees</span><span class=p>,</span><span class=w> </span><span class=n>__objs__</span><span class=p>.</span><span class=n>obj</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=n>X</span><span class=p>,</span><span class=w> </span><span class=n>__objs__</span><span class=p>.</span><span class=n>obj</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=n>y</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>__objs__</span><span class=p>.</span><span class=n>obj</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=n>trees</span><span class=p>,</span><span class=w> </span><span class=n>__objs__</span><span class=p>.</span><span class=n>obj</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=n>forest_acc</span><span class=p>,</span><span class=w> </span><span class=p>...</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>WHERE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>generate_data</span><span class=p>.</span><span class=n>output_0</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>X</span><span class=w> </span><span class=k>and</span><span class=w> </span><span class=n>generate_data</span><span class=p>.</span><span class=n>output_1</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>y</span><span class=w> </span><span class=k>and</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>train_forest</span><span class=p>.</span><span class=n>X</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>X</span><span class=w> </span><span class=k>and</span><span class=w> </span><span class=n>train_forest</span><span class=p>.</span><span class=n>y</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>y</span><span class=w> </span><span class=k>and</span><span class=w> 
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>train_forest</span><span class=p>.</span><span class=n>n_trees</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>n_trees</span><span class=w> </span><span class=k>and</span><span class=w> </span><span class=n>train_forest</span><span class=p>.</span><span class=n>output_0</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>trees</span><span class=w> </span><span class=k>and</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>eval_forest</span><span class=p>.</span><span class=n>trees</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>trees</span><span class=w> </span><span class=k>and</span><span class=w> </span><span class=n>eval_forest</span><span class=p>.</span><span class=n>X</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>X</span><span class=w> </span><span class=k>and</span><span class=w> </span><span class=n>eval_forest</span><span class=p>.</span><span class=n>y</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>y</span><span class=w> </span><span class=k>and</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>eval_forest</span><span class=p>.</span><span class=n>output_0</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>forest_acc</span><span class=w>
</span></span></span></code></pre></div><h3 id=queries-compose-with-collections-and-memoization>Queries compose with collections and memoization<a hidden class=anchor aria-hidden=true href=#queries-compose-with-collections-and-memoization>#</a></h3><p>You can propagate more complex relationships through the declarative interface,
such as ones between a collection and its elements (which allows you to query
through operations that e.g. aggregate multiple objects). Furthermore, you can
introduce pointwise constraints by directly passing objects recovered by
memoization (or even scalars):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>In</span> <span class=p>[</span><span class=mi>3</span><span class=p>]:</span> <span class=k>with</span> <span class=n>storage</span><span class=o>.</span><span class=n>run</span><span class=p>():</span>
</span></span><span class=line><span class=cl>   <span class=o>...</span><span class=p>:</span>     <span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>generate_data</span><span class=p>()</span>
</span></span><span class=line><span class=cl>   <span class=o>...</span><span class=p>:</span>     <span class=n>tree</span><span class=p>,</span> <span class=n>acc</span> <span class=o>=</span> <span class=n>train_and_eval_tree</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>seed</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=o>...</span><span class=p>:</span>     <span class=k>with</span> <span class=n>storage</span><span class=o>.</span><span class=n>query</span><span class=p>()</span> <span class=k>as</span> <span class=n>q</span><span class=p>:</span> <span class=c1># contexts can be nested</span>
</span></span><span class=line><span class=cl>   <span class=o>...</span><span class=p>:</span>         <span class=n>trees</span> <span class=o>=</span> <span class=n>Q</span><span class=p>([</span><span class=n>tree</span><span class=p>,</span> <span class=o>...</span><span class=p>])</span> <span class=c1># matches a list containing `tree`</span>
</span></span><span class=line><span class=cl>   <span class=o>...</span><span class=p>:</span>         <span class=n>forest_acc</span> <span class=o>=</span> <span class=n>eval_forest</span><span class=p>(</span><span class=n>trees</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=o>...</span><span class=p>:</span>         <span class=n>df</span> <span class=o>=</span> <span class=n>q</span><span class=o>.</span><span class=n>get_table</span><span class=p>(</span><span class=n>trees</span><span class=o>.</span><span class=n>named</span><span class=p>(</span><span class=s1>&#39;trees&#39;</span><span class=p>),</span> 
</span></span><span class=line><span class=cl>   <span class=o>...</span><span class=p>:</span>                          <span class=n>forest_acc</span><span class=o>.</span><span class=n>named</span><span class=p>(</span><span class=s1>&#39;forest_acc&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>   <span class=o>...</span><span class=p>:</span> <span class=n>df</span>
</span></span><span class=line><span class=cl><span class=n>Out</span><span class=p>[</span><span class=mi>3</span><span class=p>]:</span> 
</span></span><span class=line><span class=cl>                                               <span class=n>trees</span>  <span class=n>forest_acc</span>
</span></span><span class=line><span class=cl><span class=mi>0</span>  <span class=p>[</span><span class=n>DecisionTreeClassifier</span><span class=p>(</span><span class=n>max_depth</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>max_featu</span><span class=o>...</span>        <span class=mf>0.97</span>
</span></span><span class=line><span class=cl><span class=mi>1</span>  <span class=p>[</span><span class=n>DecisionTreeClassifier</span><span class=p>(</span><span class=n>max_depth</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>max_featu</span><span class=o>...</span>        <span class=mf>0.94</span>
</span></span><span class=line><span class=cl><span class=mi>2</span>  <span class=p>[</span><span class=n>DecisionTreeClassifier</span><span class=p>(</span><span class=n>max_depth</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>max_featu</span><span class=o>...</span>        <span class=mf>0.99</span>
</span></span><span class=line><span class=cl><span class=mi>3</span>  <span class=p>[</span><span class=n>DecisionTreeClassifier</span><span class=p>(</span><span class=n>max_depth</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>max_featu</span><span class=o>...</span>        <span class=mf>0.96</span>
</span></span></code></pre></div><p>In effect, the above &ldquo;hybrid&rdquo; query is one way to &ldquo;open up&rdquo; the abstraction
provided by the function <code>train_forest</code>, and query its internals.</p><h2 id=co-evolve-code-and-data>Co-evolve code and data<a hidden class=anchor aria-hidden=true href=#co-evolve-code-and-data>#</a></h2><p>So far, we treated memoized functions as unchanging, but that&rsquo;s quite
unrealistic. One of the thorniest problems of data management is maintaining a
clear relationship between code and data in the face of changes in the internal
logic of a project&rsquo;s building blocks:</p><ul><li>if you ignore a change in logic, you generally end up with a mixture of
results that <em>look</em> homogeneous to the system, but differ in the way they were
generated. This makes comparisons between them more or less meaningless.</li><li>if <em>every</em> change is automatically marked as a change in logic (looking at
you, <code>git</code>), executions of
semantically equivalent code will be confusingly scattered across versions.
This makes comparisons between them difficult to program, and deters users
from code quality improvements.</li></ul><p>An automatic and universal solution to this problem is
<a href=https://stackoverflow.com/a/1132167/6538618>fundamentally impossible</a>.
However, there are some easy-to-use opinionated tools to streamline the manual
process of sorting changes into those that are irrelevant, compatible with past
results, or incompatible (and thus require recomputation).</p><h3 id=extending-a-function-backward-compatibly>Extending a function backward-compatibly<a hidden class=anchor aria-hidden=true href=#extending-a-function-backward-compatibly>#</a></h3><p>A very common scenario in practice is that you have a function you&rsquo;ve run a
bunch of times, and then you get an idea for how it could do its work
differently (e.g. use a new algorithm, or vary a previously hardcoded constant).
Importantly, you want to keep the old results around, but somehow distinguish
them from the new ones.</p><p>In <code>mandala</code>, you can do this by adding an argument with a default value,
and it JustWorks™. A column is added to the current memoization table, with the
default value applied retroactively to past calls. All the memoized calls
without the new argument are still memoized. See below for how to expose a new
hyperparameter to the pipeline, and add it to the declarative query as well:</p><p><img loading=lazy src=../../videos/add_arg.mp4 alt></p><p><details open><summary markdown=span>Show/hide video</summary>
<video width=100% controls>
<source src=../../videos/add_arg.mp4 type=video/mp4><source src=../../videos/add_arg.webm type=video/webm>Your browser does not support HTML5 video.</video></details></p><h3 id=function-level-dependency-tracking>Function-level dependency tracking<a hidden class=anchor aria-hidden=true href=#function-level-dependency-tracking>#</a></h3><p>The harder problem is detecting when a function&rsquo;s dependencies have changed,
potentially rendering the memoized results out of sync with the new
codebase. Since we&rsquo;re basing everything on memoization, a purely <code>git</code>-style
versioning model is too coarse-grained to work: a change in the logic of one
memoized function should not invalidate the results of another memoized function
that doesn&rsquo;t depend on it!</p><p>A <em>dependency tracker for individual functions</em> is needed, so that only the
functions that are affected by a change can be recomputed. The problem is that a
function&rsquo;s dependencies are not just its source code and global variables, but
also, recursively, the dependencies of all functions it calls. A perfect
solution to this problem is <a href=https://www.benkuhn.net/deps/>difficult</a>.</p><p>Currently, <code>mandala</code> opts for a conservative solution that may overestimate the
set of dependencies <em>for a particular call</em> of a function, as well as the set of
global variables a function depends on. When a dependency change is detected,
you&rsquo;re presented with a diff, the memoized functions that potentially depend on
this change, and asked whether this change should be ignored (so old results are
kept) or new versions of the memoized functions should be created (in which case
old calls would be recomputed against the new codebase):</p><p><details open><summary markdown=span>Show/hide video</summary>
<video width=100% controls>
<source src=../../videos/deps.mp4 type=video/mp4><source src=../../videos/deps.webm type=video/webm>Your browser does not support HTML5 video.</video></details></p><p>This feature is still actively being designed, and there are many corner cases
in which it (loudly) fails. However, when used properly, it gives you the best
of both worlds: a tidy, comprehensible storage, and the ability to refactor code
while retaining its connection to stored results.</p><h2 id=conclusion-and-next-up>Conclusion, and next up<a hidden class=anchor aria-hidden=true href=#conclusion-and-next-up>#</a></h2><p>This was a rather brisk walk through <code>mandala</code>, but you&rsquo;ve hopefully come away
with a good sense of its general spirit and the sorts of simplicity it provides.
Key features were not mentioned here, but are currently in the works:</p><ul><li><strong>batch execution</strong>: separate data management and execution concerns in cases
when it&rsquo;s more efficient to run function calls in batches, such as inference on
large machine learning models. Code inside a <code>with storage.batch()</code> context is executed by a custom (batched) executor that you
can implement, yet each call is individually memoized and queriable.</li><li><strong>parallelization</strong>: since <code>mandala</code> is function-based, it&rsquo;s a great fit for
function-based parallelization frameworks like <a href=https://www.dask.org/><code>dask</code></a>.</li><li><strong>deletion interfaces</strong>: an imperative/declarative deletion interface,
analogous to the imperative/declarative query interfaces. Just change <code>with storage.run()</code> to <code>with storage.delete()</code>.</li></ul><p>The plan for future blog posts is to focus on those, as well as on applications
and the features described here in more depth.</p><h3 id=getting-involved>Getting involved<a hidden class=anchor aria-hidden=true href=#getting-involved>#</a></h3><p>We&rsquo;re actively looking for more early adopters excited to try out
<a href=https://github.com/amakelov/mandala><code>mandala</code></a> to manage their computational
projects! While it is still a prototype of sorts, and there&rsquo;s a lot to improve
in terms of performance, it can handle a wide array of workloads quite well.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>