<!doctype html><html lang=en dir=auto><head><link rel=stylesheet href=/style.css><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Mandala: Python programs that save, query and version themselves | Alex Makelov</title><meta name=keywords content><meta name=description content="tl;dr Your code and its execution traces contain enough information to save, query and version computations without extra boilerplate. mandala is a persistent memoization cache on steroids that lets you tap into this information. It
turns Python programs - composed of function calls, collections and control flow - into interlinked, persistent data as they execute. can be queried by directly pattern-matching against such programs, returning tables of values in the cache satisfying the computational relationships of the program tracks the dependencies of each memoized function call, automatically detecting when a change in the code may require recomputation."><meta name=author content><link rel=canonical href=https://amakelov.github.io/blog/pl/><link crossorigin=anonymous href=/assets/css/stylesheet.bccfefac377bc340f06c260aed1bddf49a4354816d7c570d6aac75a997986c95.css integrity="sha256-vM/vrDd7w0DwbCYK7Rvd9JpDVIFtfFcNaqx1qZeYbJU=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://amakelov.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://amakelov.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://amakelov.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://amakelov.github.io/apple-touch-icon.png><link rel=mask-icon href=https://amakelov.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Mandala: Python programs that save, query and version themselves"><meta property="og:description" content="tl;dr Your code and its execution traces contain enough information to save, query and version computations without extra boilerplate. mandala is a persistent memoization cache on steroids that lets you tap into this information. It
turns Python programs - composed of function calls, collections and control flow - into interlinked, persistent data as they execute. can be queried by directly pattern-matching against such programs, returning tables of values in the cache satisfying the computational relationships of the program tracks the dependencies of each memoized function call, automatically detecting when a change in the code may require recomputation."><meta property="og:type" content="article"><meta property="og:url" content="https://amakelov.github.io/blog/pl/"><meta property="og:image" content="https://amakelov.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="og:site_name" content="Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://amakelov.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Mandala: Python programs that save, query and version themselves"><meta name=twitter:description content="tl;dr Your code and its execution traces contain enough information to save, query and version computations without extra boilerplate. mandala is a persistent memoization cache on steroids that lets you tap into this information. It
turns Python programs - composed of function calls, collections and control flow - into interlinked, persistent data as they execute. can be queried by directly pattern-matching against such programs, returning tables of values in the cache satisfying the computational relationships of the program tracks the dependencies of each memoized function call, automatically detecting when a change in the code may require recomputation."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Blogs","item":"https://amakelov.github.io/blog/"},{"@type":"ListItem","position":3,"name":"Mandala: Python programs that save, query and version themselves","item":"https://amakelov.github.io/blog/pl/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Mandala: Python programs that save, query and version themselves","name":"Mandala: Python programs that save, query and version themselves","description":"tl;dr Your code and its execution traces contain enough information to save, query and version computations without extra boilerplate. mandala is a persistent memoization cache on steroids that lets you tap into this information. It\nturns Python programs - composed of function calls, collections and control flow - into interlinked, persistent data as they execute. can be queried by directly pattern-matching against such programs, returning tables of values in the cache satisfying the computational relationships of the program tracks the dependencies of each memoized function call, automatically detecting when a change in the code may require recomputation.","keywords":[],"articleBody":"tl;dr Your code and its execution traces contain enough information to save, query and version computations without extra boilerplate. mandala is a persistent memoization cache on steroids that lets you tap into this information. It\nturns Python programs - composed of function calls, collections and control flow - into interlinked, persistent data as they execute. can be queried by directly pattern-matching against such programs, returning tables of values in the cache satisfying the computational relationships of the program tracks the dependencies of each memoized function call, automatically detecting when a change in the code may require recomputation. This radically simplifies data management for computational artifacts, leading to efficient reuse of computations, clear code, fewer bugs, easier reproducibility, and improved developer productivity. The project is still in active development, and not optimized for performance. Its primary use case is data science and machine learning experiments.\nIntroduction They finally figured out how the Roman Empire actually fell, and it was this file: model_param=0.1_best_VERSION2_preprocess=True_final_FIXED.pkl. …OK, we all agree this filename’s terrible - but why exactly? Its job is to tell the story of a computation. But computations are more expressive, structured, and malleable than filenames. So, it’s no wonder you run into awkwardness, confusion and bugs when you try to turn one into the other. This impedance mismatch is at the root of the computational data management problem.\nBy contrast, the code producing the file’s contents is a far superior telling of its story:\nX, y = load_data() if preprocess: X = preprocess_data(X) model = train_v2(X, y, param=0.1) It makes it clear how logic and parameters combine to produce model. More subtly, implicit in the code are the invariant-across-executions computational relationships that we typically want to query (e.g., “find all the models trained on a particular dataset X”). This raises a question:\nIf the code already contains all this structured information about the meaning of its results, why not just make it its own storage interface?\nmandala is a tool that implements this idea. It turns function calls into interlinked, queriable, content-versioned data that is automatically co-evolved with your codebase. By applying different semantics to this data, the same ordinary Python code, including control flow and collections, can be used to not only compute, but also save, load, query, batch-compute, and combinations of those.\nUnlike existing frameworks addressing various parts of the problem, mandala bakes data management logic deeper into Python itself, and intentionally has few interfaces, concepts, and domain-specific features to speak of. Instead, it lets you do your work mostly through plain code. This direct approach makes it much more natural and less error-prone to iteratively compose and query complex experiments.\nOutline This blog post is a brief, somewhat programming-languages-themed tour of the unusual core features that work together to make this possible:\nmemoization aligned with core Python features turns programs into imperative computation/storage/query interfaces and enables incremental computing by default; pattern-matching queries turn programs into declarative query interfaces to analogous computations in the entire storage; per-call versioning and dependency tracking enable further fine-grained incremental computation by tracking changes in the dependencies of each memoized call. There is a companion notebook for this blog post that you can read here or Python-integrated memoization While memoization is typically used as a mere cache, it can be much more. Imagine a complex piece of code where every function call has been memoized in a shared cache. Such code effectively becomes a map of the cache that you can traverse by (efficiently) retracing its steps, no matter how complex the logic. You can also incrementally build computations by just adding to this code: you get through the memoized part fast, and do the new work. This is especially useful in interactive environments like Jupyter notebooks.\nCollections, lazy reads, and control flow To illustrate, here’s a simple machine learning pipeline for a do-it-yourself random forest classifier:\n@op # core mandala decorator def generate_data() -\u003e Tuple[ndarray, ndarray]: ... @op def train_and_eval_tree(X, y, seed) -\u003e Tuple[DecisionTreeClassifier, float]: ... @op def eval_forest(trees:List[DecisionTreeClassifier], X, y) -\u003e float: # get majority vote accuracy ... We can compose these functions in the obvious way to train a few trees and evaluate the resulting forest:\nwith storage.run(): # memoization context manager X, y = generate_data() trees = [] for seed in range(10): # can't grow trees without seeds :) tree, acc = train_and_eval_tree(X, y, seed=seed) trees.append(tree) forest_acc = eval_forest(trees, X, y) Note that we are passing the list trees of memoized values into another memoized function. This incurs no storage duplication, because each element of the list is stored separately, and trees behaves like a list of pointers to storage locations, rather than a monolithic blob. This is one example of how mandala’s memoization JustWorks™ with Python’s collections.\nThis compatibility lets you write algorithms that naturally operate on collections of objects (which do come up in practice, e.g. aggregators, model ensembling, clustering, chunking, …) in the most straightforward way, while still having intermediate results cached (and even declaratively queriable). The video below shows this in action by incrementally evolving the workflow above with new logic and parameters:\nShow/hide video Your browser does not support HTML5 video. The video above illustrates\nmemoization as imperative query interface: you can get to any of the computed quantities just by retracing the memoized code, or parts of it, again. laziness: when run against a persistent storage, mandala only reads from disk when a value is needed for a new computation control flow: laziness is designed to do the least work compatible with control flow - even though initially acc is a lazy reference, when the if acc \u003e 0.8: statement is reached, this causes a read from the storage. This applies more generally to collections: a lazy reference to a collection has no elements, but e.g. iterating over it automatically loads (lazy) references to its elements. Hierarchical memoization and (more) incremental computing There’s still a major scalability problem - what if we had a workflow involving not tens but thousands or even millions of function calls? Stepping through each memoized call to get to a single final result (like we do with calls to train_and_eval_tree here) - even with laziness - could take a long time.\nThe solution is simple: memoize large subprograms like that end-to-end by abstracting them as a single (parametrized) memoized function. After all, every time there is a large number of calls happening, they share some repetitive structure that can be abstracted away. This way you can take “shortcuts” in the memoization graph:\nShow/hide video Your browser does not support HTML5 video. The @superop decorator you see in the video is needed to indicate that a memoized function is itself a composition of memoized functions. A few of the nice things enabled by this:\nshortcutting via memoization: the first time when you run the refactored workflow, execution goes inside the body of train_forest and loops over all the memoized calls to train_and_eval_tree, but the second time it has already memoized the call to train_forest end-to-end and jumps right to the return value. That’s why the second run is faster! natively incremental computing: calling train_forest with a higher value of n_trees does not do all its work from scratch: since it internally uses memoized functions, it is able to (lazily) skip over the memoized calls and do only the new work. Pattern-matching queries Using memoization as a query interface relies on you knowing the “initial conditions” from which to start stepping through the trail of memoized calls - but this is not always a good fit. As a complement to this imperative interface, you also have a declarative query interface over the entire storage. To illustrate, consider where we might have left things with our random forest example:\nIn [23]: with storage.run(): ...: X, y = generate_data() ...: for n_trees in (5, 10, ): ...: trees = train_forest(X, y, n_trees) ...: forest_acc = eval_forest(trees, X, y) If this project went on for a while, you may not remember all the values of n_trees you ran this with. Fortunately, this code contains the repeated relationship between X, y, n_trees, trees, forest_acc you care about. In fact, the values of these local variables after this code executes are in exactly this relationship. Since every @op-decorated call links its inputs and outputs behind the scenes, you can inspect the qualitative history of forest_acc in this computation as a graph by calling storage.draw_graph(forest_acc):\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?\u003e \u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\" \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e G 140658408547392 forest_acc 140656954281072 y 140656891611744 X y n_trees train_forest(X, y, n_trees) output_0 140656954281072-\u003e140656891611744:y 140658408549984 trees X y eval_forest(trees, X, y) output_0 140656954281072-\u003e140658408549984:y 140656891622544 trees 140656891622544-\u003e140658408549984:trees 140656954280256 X 140656954280256-\u003e140656891611744:X 140656954280256-\u003e140658408549984:X 140656891618224 a0 140656891618224-\u003e140656891611744:n_trees 140656891611744:output_0-\u003e140656891622544 140658408549984:output_0-\u003e140658408547392 140656954282464 generate_data() output_0 output_1 140656954282464:output_1-\u003e140656954281072 140656954282464:output_0-\u003e140656954280256 This data makes it possible to directly point to the variable forest_acc and issue a query for all values in the storage that have the same qualitative computational history:\nIn [42]: storage.similar(forest_acc, context=True) Pattern-matching to the following computational graph (all constraints apply): a0 = Q() # input to computation; can match anything X, y = generate_data() trees = train_forest(X=X, y=y, n_trees=a0) forest_acc = eval_forest(trees=trees, X=X, y=y) result = storage.df(a0, y, X, trees, forest_acc) Out[42]: a0 y X trees forest_acc 2 5 [0, 1, ... [[0.0, 0.0, ... [DecisionTreeC... 0.96 3 10 [0, 1, ... [[0.0, 0.0, ... [DecisionTreeC... 0.99 0 15 [0, 1, ... [[0.0, 0.0, ... [DecisionTreeC... 0.99 1 20 [0, 1, ... [[0.0, 0.0, ... [DecisionTreeC... 0.99 The context=True option includes the values forest_acc depends on in the table. In particular, this reveals all the values of n_trees this workflow was ran with, and the intermediate results along the way.\nFinally, you have an explicit counterpart to this implicit query interface: if you copy-paste the code for the computational graph above into a with storage.query(): block, result will be the same table as above.\nWhat just happened? 🤯 Behind the scenes, the computational graph that forest_acc is derived from gets compiled to a SQL query over memoization tables. In the returned table, each row is a matching of values to the variables that satisfies all the computational relationships in the graph. This is the classical database concept of conjunctive queries.\nAnd here’s (simplified) SQL code the pattern-matching query compiles to, where you should think of __objs__ as a table of all the Python objects in the storage:\nSELECT a0, y, X, trees, forest_acc FROM __objs__.obj as a0, __objs__.obj as X, __objs__.obj as y, __objs__.obj as trees, __objs__.obj as forest_acc, ... WHERE generate_data.output_0 = X and generate_data.output_1 = y and train_forest.X = X and train_forest.y = y and train_forest.a0 = a0 and train_forest.output_0 = trees and eval_forest.trees = trees and eval_forest.X = X and eval_forest.y = y and eval_forest.output_0 = forest_acc Pattern-matching with collections: the color refinement projection You can propagate more complex relationships through the declarative interface, such as ones between a collection and its elements. This allows you to query programs that combine multiple objects in a single result (e.g., data aggregation, model ensembling, processing data in chunks, …) and/or generate a variable number of objects (e.g., clustering, …).\nTo illustrate, suppose that just for fun we disrupt the nice flow of our program by slicing into the list of trees to evaluate only on the first half:\nwith storage.run(): X, y = generate_data() for n_trees in wrap((10, 15, 20)): trees = train_forest(X, y, n_trees) forest_acc = eval_forest(trees[:n_trees // 2], X, y) If you look at how forest_acc is computed now, you get a substantially larger graph:\nThe problem is that there are now six (apparently, for n_trees=20, 12 or 13 trees survived the filtering!) chains of “get an element from trees, put it in a new list” computations in this graph that are redundant (they look exactly the same) and at the same time tie the graph to a particular value of n_trees. This will make the query both slow and too specific for our needs.\nThere is a principled solution to eliminate such repetition from any computational graph: a modified color refinement algorithm. There’s too many details for the scope of this blog post, but the overall intuition is that you can project1 any computational graph to a smaller graph where there are no two vertices that “look the same” in the context of the entire computation2. Here is what you get when you run storage.draw_graph(forest_acc, project=True):\nAnd here is the code for this graph that you get using storage.print_graph(forest_acc, project=True):\nidx0 = Q() # index into list idx1 = Q() # index into list X, y = generate_data() n_trees = Q() # input to computation; can match anything trees = train_forest(X=X, y=y, n_trees=n_trees) a0 = trees[idx1] # a0 will match any element of a match for trees at index matching idx1 a1 = ListQ(elts=[a0], idxs=[idx0]) # a1 will match any list containing a match for a0 at index idx0 forest_acc = eval_forest(trees=a1, X=X, y=y) Note how the two indices idx0, idx1 are different variables. This means that the result of this query will be larger than it intuitively needs to be, because there will be matches for one index in trees and another index in trees[:n_trees//2]. You can restrict it further by making idx0, idx1 the same variable.\nPer-call versioning and dependency tracking So far, we’ve treated memoized functions as unchanging, but that’s quite unrealistic. One of the thorniest problems of data management is maintaining a clear relationship between code and data in the face of changes in the internal logic of a project’s building blocks:\nif you ignore a change in logic, you generally end up with a mixture of results that look homogeneous to the system, but differ in the way they were generated. This makes comparisons between them more or less meaningless. if every change is automatically marked as a change in logic (looking at you, git), executions of semantically equivalent code will be confusingly scattered across versions. This makes comparisons between them difficult to program, and deters users from code quality improvements. An automatic and universal solution to this problem is fundamentally impossible. What mandala offers instead is:\nper-call dependency tracking: automatically track the functions and global variables accessed by each memoized call, and alert you to changes in them, so you can (carefully) choose whether a change to a dependency requires recomputation of dependent calls content-addressable versions: use the current state of each dependency in your codebase to automatically determine the currently compatible versions of each memoized function to use in computation and queries. In particular, this means that: you can go “back in time” and access the storage relative to an earlier state of the code (or even branch away in a new direction like in git) by just restoring this state the code is the truth: when in doubt about the meaning of a result, you can just look at the current code. Warmup: adding an argument backward-compatibly A very common type of change in practice is to have a function you’ve run a few times, and then get an idea for how it could do its work differently (e.g. use a new algorithm, or vary a previously hardcoded constant). Importantly, you want to keep the old results around, but also somehow distinguish them from the new ones.\nIn mandala, you can do this by adding an argument with a default value, and it JustWorks™. A column is added to the function’s memoization table, with the default value applied retroactively to past calls. All the memoized calls without the new argument are still memoized. When doing this with a versioned storage, you’ll be shown a diff and should mark the change as not requiring recomputation of dependents:\nShow/hide video Your browser does not support HTML5 video. Marking changes as breaking and traveling back in time When you discover a bug affecting results, or just want to change how a function works and recompute everything that depends on it, you mark the change as requiring recomputation in the diff dialog. This will essentially tell the storage that this function now “means something else” (under the hood, it has a different semantic id from the previous version).\nIf you want to revisit the old results, restore the code to the old state; this will cause the storage to interpret the function according to its previous meaning in both computation and queries. You can examine the “shallow” versions of a dependency (i.e. the revisions of its own source code) with storage.sources(f), and the “deep” versions (which include sets of dependencies as seen by calls to this function) with storage.versions(f):\nShow/hide video Your browser does not support HTML5 video. Conclusion This was a rather brisk walk through mandala, but it hopefully gave you an idea of its general spirit and the particular ways in which it simplifies computational data management. Some features that were not mentioned here, but are currently in the works:\nbatch execution: separate data management and execution concerns in cases when it’s more efficient to run function calls in batches, such as inference on large machine learning models. Code inside a with storage.batch() context is executed by a custom (batched) executor that you can implement, yet each call is individually memoized and queriable. parallelization: since mandala is function-based, it’s a great fit for function-based parallelization frameworks like dask. deletion interfaces: an imperative/declarative deletion interface, analogous to the imperative/declarative query interfaces. Just change with storage.run() to with storage.delete(). We invite you to try mandala for your own projects and welcome contributions to help improve its performance and capabilities!\nAcknowledgements Thanks to Stefan Krastanov, Melody Guan and Ben Kuhn for providing feedback on drafts of this post.\nin the sense of a surjective labeled graph homomorphism ↩︎\nthis works in almost all practical cases. However, there exist instances in which the color refinement algorithm fails to distinguish two vertices in the graph that do in fact have a differen role in the computation. When in doubt, read the code for the computational graph printed during queries to make sure it captures the relationships you want to capture. ↩︎\n","wordCount":"2994","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://amakelov.github.io/blog/pl/"},"publisher":{"@type":"Organization","name":"Alex Makelov","logo":{"@type":"ImageObject","url":"https://amakelov.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://amakelov.github.io accesskey=h title="Alex Makelov (Alt + H)">Alex Makelov</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Mandala: Python programs that save, query and version themselves</h1><div class=post-meta></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#tldr>tl;dr</a></li><li><a href=#introduction>Introduction</a><ul><li><a href=#outline>Outline</a></li></ul></li><li><a href=#python-integrated-memoization>Python-integrated memoization</a><ul><li><a href=#collections-lazy-reads-and-control-flow>Collections, lazy reads, and control flow</a></li><li><a href=#hierarchical-memoization-and-more-incremental-computing>Hierarchical memoization and (more) incremental computing</a></li></ul></li><li><a href=#pattern-matching-queries>Pattern-matching queries</a><ul><li><a href=#what-just-happened->What just happened? 🤯</a></li><li><a href=#pattern-matching-with-collections-the-color-refinement-projection>Pattern-matching with collections: the color refinement projection</a></li></ul></li><li><a href=#per-call-versioning-and-dependency-tracking>Per-call versioning and dependency tracking</a><ul><li><a href=#warmup-adding-an-argument-backward-compatibly>Warmup: adding an argument backward-compatibly</a></li><li><a href=#marking-changes-as-breaking-and-traveling-back-in-time>Marking changes as breaking and traveling back in time</a></li></ul></li><li><a href=#conclusion>Conclusion</a><ul><li><a href=#acknowledgements>Acknowledgements</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h2 id=tldr>tl;dr<a hidden class=anchor aria-hidden=true href=#tldr>#</a></h2><p>Your code and its execution traces contain enough information to save, query and
version computations without extra boilerplate. <a href=https://github.com/amakelov/mandala><code>mandala</code></a>
is a persistent memoization cache on steroids that lets you tap into this
information. It</p><ul><li>turns Python programs - composed of function calls, collections and control
flow - into interlinked, persistent data as they execute.</li><li>can be queried by directly pattern-matching against such programs, returning
tables of values in the cache satisfying the computational relationships
of the program</li><li>tracks the dependencies of each memoized function call, automatically
detecting when a change in the code may require recomputation.</li></ul><p>This radically simplifies data management for computational artifacts, leading
to efficient reuse of computations, clear code, fewer bugs, easier
reproducibility, and improved developer productivity. The project is still in
active development, and not optimized for performance. Its primary use case is
data science and machine learning experiments.</p><p><img loading=lazy src=/videos/tldr.gif alt=preview></p><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>They finally figured out how the Roman Empire <em>actually</em> fell, and it was
this file: <code>model_param=0.1_best_VERSION2_preprocess=True_final_FIXED.pkl</code>. &mldr;OK,
we all agree this filename&rsquo;s terrible - but why exactly? Its job is to tell the
story of a computation. But computations are more expressive, structured,
and malleable than filenames. So, it&rsquo;s no wonder you run into awkwardness,
confusion and bugs when you try to turn one into the other. This impedance
mismatch is at the root of the <strong>computational data management problem</strong>.</p><p>By contrast, the code producing the file&rsquo;s contents is a far superior telling of
its story:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>load_data</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>preprocess</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>X</span> <span class=o>=</span> <span class=n>preprocess_data</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>train_v2</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>param</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>
</span></span></code></pre></div><p>It makes it clear how logic and parameters combine to produce <code>model</code>.
More subtly, implicit in the code are the invariant-across-executions
computational relationships that we typically want to query (e.g., &ldquo;find all the
models trained on a particular dataset <code>X</code>&rdquo;). This raises a question:</p><blockquote><p><em>If the code already contains all this structured information about the
meaning of its results, why not just make it its own storage interface?</em></p></blockquote><p><a href=https://github.com/amakelov/mandala><code>mandala</code></a> is a tool that implements this
idea. It turns function calls into interlinked, queriable, content-versioned data that
is automatically co-evolved with your codebase. By applying different semantics
to this data, the same ordinary Python code, including control flow
and collections, can be used to not only compute, but also save, load, query,
batch-compute, and combinations of those.</p><p><a href=https://wandb.ai/site>Unlike</a> <a href=https://github.com/IDSIA/sacred>existing</a>
<a href=https://mlflow.org/>frameworks</a> <a href=https://github.com/iterative/dvc>addressing</a>
various parts of the problem,
<code>mandala</code> bakes data management logic deeper into Python itself, and
intentionally has few interfaces, concepts, and domain-specific features to
speak of. Instead, it lets you do your work mostly through plain code. This
direct approach makes it much more natural and less error-prone to iteratively
compose and query complex experiments.</p><h3 id=outline>Outline<a hidden class=anchor aria-hidden=true href=#outline>#</a></h3><p>This blog post is a brief, somewhat programming-languages-themed tour of
the unusual core features that work together to make this possible:</p><ul><li><a href=#python-integrated-memoization>memoization</a> aligned with core Python
features turns programs into <em>imperative</em> computation/storage/query interfaces
and enables incremental computing by default;</li><li><a href=#pattern-matching-queries>pattern-matching queries</a> turn programs into
<em>declarative</em> query interfaces to analogous computations in the entire storage;</li><li><a href=#per-call-versioning-and-dependency-tracking>per-call versioning and dependency
tracking</a> enable further
fine-grained incremental computation by tracking changes in the dependencies of
each memoized call.</li></ul><p>There is a companion notebook for this blog post that you can <a href=https://github.com/amakelov/mandala/blob/master/tutorials/03_advanced.ipynb>read
here</a>
or<div align=center><a href=https://colab.research.google.com/github/amakelov/mandala/blob/master/tutorials/03_advanced.ipynb><img src=https://colab.research.google.com/assets/colab-badge.svg alt="Open In Colab"></a></div></p><h2 id=python-integrated-memoization>Python-integrated memoization<a hidden class=anchor aria-hidden=true href=#python-integrated-memoization>#</a></h2><p>While <a href=https://en.wikipedia.org/wiki/Memoization>memoization</a> is typically used
as a mere cache, it can be much more. Imagine a complex piece of code where
<em>every</em> function call has been memoized in a shared cache. Such code effectively
becomes a map of the cache that you can traverse by (efficiently) retracing its
steps, no matter how complex the logic. You can also incrementally build
computations by just adding to this code: you get through the memoized part
fast, and do the new work. This is especially useful in interactive environments
like <a href=https://jupyter.org/>Jupyter notebooks</a>.</p><h3 id=collections-lazy-reads-and-control-flow>Collections, lazy reads, and control flow<a hidden class=anchor aria-hidden=true href=#collections-lazy-reads-and-control-flow>#</a></h3><p>To illustrate, here&rsquo;s a simple machine learning pipeline for a do-it-yourself
random forest classifier:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nd>@op</span> <span class=c1># core mandala decorator</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>generate_data</span><span class=p>()</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>ndarray</span><span class=p>,</span> <span class=n>ndarray</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@op</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>train_and_eval_tree</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>seed</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>Tuple</span><span class=p>[</span><span class=n>DecisionTreeClassifier</span><span class=p>,</span> <span class=nb>float</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl><span class=nd>@op</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>eval_forest</span><span class=p>(</span><span class=n>trees</span><span class=p>:</span><span class=n>List</span><span class=p>[</span><span class=n>DecisionTreeClassifier</span><span class=p>],</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>float</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># get majority vote accuracy</span>
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span></code></pre></div><p>We can compose these functions in the obvious way to train a few trees and
evaluate the resulting forest:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>with</span> <span class=n>storage</span><span class=o>.</span><span class=n>run</span><span class=p>():</span> <span class=c1># memoization context manager</span>
</span></span><span class=line><span class=cl>    <span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>generate_data</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>trees</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>seed</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>10</span><span class=p>):</span> <span class=c1># can&#39;t grow trees without seeds :)</span>
</span></span><span class=line><span class=cl>        <span class=n>tree</span><span class=p>,</span> <span class=n>acc</span> <span class=o>=</span> <span class=n>train_and_eval_tree</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>seed</span><span class=o>=</span><span class=n>seed</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>trees</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>tree</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>forest_acc</span> <span class=o>=</span> <span class=n>eval_forest</span><span class=p>(</span><span class=n>trees</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span></code></pre></div><p>Note that we are passing the list <code>trees</code> of memoized values into another
memoized function. This incurs no storage duplication, because each element of
the list is stored separately, and <code>trees</code> behaves like a list of pointers to
storage locations, rather than a monolithic blob. This is one example of how
<code>mandala</code>&rsquo;s memoization JustWorks™ with Python&rsquo;s collections.</p><p>This compatibility lets you write algorithms that naturally operate on
collections of objects (which <em>do</em> come up in practice, e.g. aggregators, model
ensembling, clustering, chunking, &mldr;) in the most straightforward way, while
still having intermediate results cached (and even <a href=#pattern-matching-with-collections-the-color-refinement-projection>declaratively
queriable</a>). The video below shows this in action by
incrementally evolving the workflow above with new logic and parameters:</p><p><details open><summary markdown=span>Show/hide video</summary>
<video width=100% controls>
<source src=../../videos/mem.mp4 type=video/mp4><source src=../../videos/mem.webm type=video/webm>Your browser does not support HTML5 video.</video></details></p><p>The video above illustrates</p><ul><li><strong>memoization as imperative query interface</strong>: you can get to any of the
computed quantities just by retracing the memoized code, or parts of it, again.</li><li><strong>laziness</strong>: when run against a persistent storage, <code>mandala</code> only reads from
disk when a value is needed for a new computation</li><li><strong>control flow</strong>: laziness is designed to do the least work compatible with
control flow - even though initially <code>acc</code> is a lazy reference, when the <code>if acc > 0.8:</code> statement is reached, this causes a read from the storage. This
applies more generally to collections: a lazy reference to a collection has no
elements, but e.g. iterating over it automatically loads (lazy) references to its
elements.</li></ul><h3 id=hierarchical-memoization-and-more-incremental-computing>Hierarchical memoization and (more) incremental computing<a hidden class=anchor aria-hidden=true href=#hierarchical-memoization-and-more-incremental-computing>#</a></h3><p>There&rsquo;s still a major scalability problem - what if we had a workflow involving
not tens but thousands or even millions of function calls? Stepping through each
memoized call to get to a single final result (like we do with calls to
<code>train_and_eval_tree</code> here) - even with laziness - could take a long time.</p><p>The solution is simple: memoize large subprograms like that end-to-end by
abstracting them as a single (parametrized) memoized function. After all, <strong>every
time there is a large number of calls happening, they share some repetitive
structure that can be abstracted away</strong>. This way you can take &ldquo;shortcuts&rdquo; in the
memoization graph:</p><p><details open><summary markdown=span>Show/hide video</summary>
<video width=100% controls>
<source src=../../videos/sup.mp4 type=video/mp4><source src=../../videos/sup.webm type=video/webm>Your browser does not support HTML5 video.</video></details></p><p>The <code>@superop</code> decorator you see in the video is needed to indicate that a
memoized function is itself a composition of memoized functions. A few of the
nice things enabled by this:</p><ul><li><strong>shortcutting via memoization</strong>: the first time when you run the refactored
workflow, execution goes inside the body of <code>train_forest</code> and loops over all
the memoized calls to <code>train_and_eval_tree</code>, but the second time it has already
memoized the call to <code>train_forest</code> end-to-end and jumps right to the return
value. That&rsquo;s why the second run is faster!</li><li><strong>natively incremental computing</strong>: calling <code>train_forest</code> with a higher value of
<code>n_trees</code> does not do all its work from scratch: since it internally uses
memoized functions, it is able to (lazily) skip over the memoized calls and do
only the new work.</li></ul><h2 id=pattern-matching-queries>Pattern-matching queries<a hidden class=anchor aria-hidden=true href=#pattern-matching-queries>#</a></h2><p>Using memoization as a query interface relies on you knowing the &ldquo;initial
conditions&rdquo; from which to start stepping through the trail of memoized calls -
but this is not always a good fit. As a complement to this imperative interface,
you also have a <em>declarative</em> query interface over the entire storage. To
illustrate, consider where we might have left things with our random forest
example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>In</span> <span class=p>[</span><span class=mi>23</span><span class=p>]:</span> <span class=k>with</span> <span class=n>storage</span><span class=o>.</span><span class=n>run</span><span class=p>():</span>
</span></span><span class=line><span class=cl>   <span class=o>...</span><span class=p>:</span>     <span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>generate_data</span><span class=p>()</span>
</span></span><span class=line><span class=cl>   <span class=o>...</span><span class=p>:</span>     <span class=k>for</span> <span class=n>n_trees</span> <span class=ow>in</span> <span class=p>(</span><span class=mi>5</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=p>):</span>
</span></span><span class=line><span class=cl>   <span class=o>...</span><span class=p>:</span>         <span class=n>trees</span> <span class=o>=</span> <span class=n>train_forest</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>n_trees</span><span class=p>)</span>
</span></span><span class=line><span class=cl>   <span class=o>...</span><span class=p>:</span>         <span class=n>forest_acc</span> <span class=o>=</span> <span class=n>eval_forest</span><span class=p>(</span><span class=n>trees</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span></code></pre></div><p>If this project went on for a while, you may not remember all the values of
<code>n_trees</code> you ran this with. Fortunately, this code contains the repeated
relationship between <code>X, y, n_trees, trees, forest_acc</code> you care about. In fact,
the values of these local variables after this code executes are in exactly this
relationship. Since every <code>@op</code>-decorated call links its inputs and outputs
behind the scenes, you can inspect the qualitative history of <code>forest_acc</code> in
this computation as a graph by calling <code>storage.draw_graph(forest_acc)</code>:</p><figure align=center class=align-center><!doctype html><svg width="286pt" height="375pt" viewBox="0 0 286.38 375" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 371)"><title>G</title><polygon fill="#fff" stroke="transparent" points="-4,4 -4,-371 282.38,-371 282.38,4 -4,4"/><g id="node1" class="node"><title>140658408547392</title><polygon fill="#cb4b16" stroke="transparent" points="37.5,0.5 37.5,-16.5 101.5,-16.5 101.5,0.5 37.5,0.5"/><polygon fill="none" stroke="#002b36" points="37.5,0.5 37.5,-16.5 101.5,-16.5 101.5,0.5 37.5,0.5"/><text text-anchor="start" x="40.5" y="-6.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#fdf6e3">forest_acc</text></g><g id="node2" class="node"><title>140656954281072</title><polygon fill="#cb4b16" stroke="transparent" points="229.5,-279.5 229.5,-296.5 242.5,-296.5 242.5,-279.5 229.5,-279.5"/><polygon fill="none" stroke="#002b36" points="229.5,-279.5 229.5,-296.5 242.5,-296.5 242.5,-279.5 229.5,-279.5"/><text text-anchor="start" x="232.5" y="-286.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#fdf6e3">y</text></g><g id="node6" class="node"><title>140656891611744</title><polygon fill="#fdf6e3" stroke="transparent" points="111.5,-226.5 111.5,-243.5 163.5,-243.5 163.5,-226.5 111.5,-226.5"/><polygon fill="none" stroke="#002b36" points="111.5,-226.5 111.5,-243.5 163.5,-243.5 163.5,-226.5 111.5,-226.5"/><text text-anchor="start" x="133.5" y="-233.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#002b36">X</text><polygon fill="#fdf6e3" stroke="transparent" points="163.5,-226.5 163.5,-243.5 195.5,-243.5 195.5,-226.5 163.5,-226.5"/><polygon fill="none" stroke="#002b36" points="163.5,-226.5 163.5,-243.5 195.5,-243.5 195.5,-226.5 163.5,-226.5"/><text text-anchor="start" x="176" y="-233.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#002b36">y</text><polygon fill="#fdf6e3" stroke="transparent" points="195.5,-226.5 195.5,-243.5 262.5,-243.5 262.5,-226.5 195.5,-226.5"/><polygon fill="none" stroke="#002b36" points="195.5,-226.5 195.5,-243.5 262.5,-243.5 262.5,-226.5 195.5,-226.5"/><text text-anchor="start" x="208" y="-233.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#002b36">n_trees</text><polygon fill="#268bd2" stroke="transparent" points="111.5,-209.5 111.5,-226.5 262.5,-226.5 262.5,-209.5 111.5,-209.5"/><polygon fill="none" stroke="#002b36" points="111.5,-209.5 111.5,-226.5 262.5,-226.5 262.5,-209.5 111.5,-209.5"/><text text-anchor="start" x="114.5" y="-216.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#fdf6e3">train_forest(X, y, n_trees)</text><polygon fill="#fdf6e3" stroke="transparent" points="111.5,-192.5 111.5,-209.5 262.5,-209.5 262.5,-192.5 111.5,-192.5"/><polygon fill="none" stroke="#002b36" points="111.5,-192.5 111.5,-209.5 262.5,-209.5 262.5,-192.5 111.5,-192.5"/><text text-anchor="start" x="162.5" y="-199.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#002b36">output_0</text></g><g id="edge2" class="edge"><title>140656954281072->140656891611744:y</title><path fill="none" stroke="#002b36" d="M228.63-283.93C217.07-277.75 194-264.61 184.14-253.27"/><polygon fill="#002b36" stroke="#002b36" points="187.11,-251.38 179.5,-244 180.85,-254.51 187.11,-251.38"/></g><g id="node7" class="node"><title>140658408549984</title><polygon fill="#fdf6e3" stroke="transparent" points="0.5,-86.5 0.5,-103.5 73.5,-103.5 73.5,-86.5 0.5,-86.5"/><polygon fill="none" stroke="#002b36" points="0.5,-86.5 0.5,-103.5 73.5,-103.5 73.5,-86.5 0.5,-86.5"/><text text-anchor="start" x="22" y="-93.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#002b36">trees</text><polygon fill="#fdf6e3" stroke="transparent" points="73.5,-86.5 73.5,-103.5 105.5,-103.5 105.5,-86.5 73.5,-86.5"/><polygon fill="none" stroke="#002b36" points="73.5,-86.5 73.5,-103.5 105.5,-103.5 105.5,-86.5 73.5,-86.5"/><text text-anchor="start" x="85.5" y="-93.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#002b36">X</text><polygon fill="#fdf6e3" stroke="transparent" points="105.5,-86.5 105.5,-103.5 137.5,-103.5 137.5,-86.5 105.5,-86.5"/><polygon fill="none" stroke="#002b36" points="105.5,-86.5 105.5,-103.5 137.5,-103.5 137.5,-86.5 105.5,-86.5"/><text text-anchor="start" x="118" y="-93.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#002b36">y</text><polygon fill="#268bd2" stroke="transparent" points="0.5,-69.5 0.5,-86.5 137.5,-86.5 137.5,-69.5 0.5,-69.5"/><polygon fill="none" stroke="#002b36" points="0.5,-69.5 0.5,-86.5 137.5,-86.5 137.5,-69.5 0.5,-69.5"/><text text-anchor="start" x="3.5" y="-76.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#fdf6e3">eval_forest(trees, X, y)</text><polygon fill="#fdf6e3" stroke="transparent" points="0.5,-52.5 0.5,-69.5 137.5,-69.5 137.5,-52.5 0.5,-52.5"/><polygon fill="none" stroke="#002b36" points="0.5,-52.5 0.5,-69.5 137.5,-69.5 137.5,-52.5 0.5,-52.5"/><text text-anchor="start" x="44.5" y="-59.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#002b36">output_0</text></g><g id="edge7" class="edge"><title>140656954281072->140658408549984:y</title><path fill="none" stroke="#002b36" d="M242.14-282.17C250.83-274.71 265.65-260.23 271.5-244 279.18-222.67 281.75-213.22 271.5-193c-31.58 62.31-57.2 94.42-123 97.29"/><polygon fill="#002b36" stroke="#002b36" points="148.57,-92.21 138.5,-95.5 148.42,-99.21 148.57,-92.21"/></g><g id="node3" class="node"><title>140656891622544</title><polygon fill="#cb4b16" stroke="transparent" points="18.5,-139.5 18.5,-156.5 54.5,-156.5 54.5,-139.5 18.5,-139.5"/><polygon fill="none" stroke="#002b36" points="18.5,-139.5 18.5,-156.5 54.5,-156.5 54.5,-139.5 18.5,-139.5"/><text text-anchor="start" x="21.5" y="-146.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#fdf6e3">trees</text></g><g id="edge5" class="edge"><title>140656891622544->140658408549984:trees</title><path fill="none" stroke="#002b36" d="M36.5-139.8c0 6.39000000000002.0 15.94.0 25.53"/><polygon fill="#002b36" stroke="#002b36" points="40,-114 36.5,-104 33,-114 40,-114"/></g><g id="node4" class="node"><title>140656954280256</title><polygon fill="#cb4b16" stroke="transparent" points="130.5,-279.5 130.5,-296.5 144.5,-296.5 144.5,-279.5 130.5,-279.5"/><polygon fill="none" stroke="#002b36" points="130.5,-279.5 130.5,-296.5 144.5,-296.5 144.5,-279.5 130.5,-279.5"/><text text-anchor="start" x="133.5" y="-286.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#fdf6e3">X</text></g><g id="edge1" class="edge"><title>140656954280256->140656891611744:X</title><path fill="none" stroke="#002b36" d="M137.5-279.8c0 6.38999999999999.0 15.94.0 25.53"/><polygon fill="#002b36" stroke="#002b36" points="141,-254 137.5,-244 134,-254 141,-254"/></g><g id="edge6" class="edge"><title>140656954280256->140658408549984:X</title><path fill="none" stroke="#002b36" d="M130.48-281.56C121.94-273.88 107.89-259.6 101.5-244c-22.31 54.52-13.21 75.19-12.1 129.78"/><polygon fill="#002b36" stroke="#002b36" points="92.9,-114.04 89.5,-104 85.9,-113.96 92.9,-114.04"/></g><g id="node5" class="node"><title>140656891618224</title><polygon fill="#cb4b16" stroke="transparent" points="190.5,-279.5 190.5,-296.5 210.5,-296.5 210.5,-279.5 190.5,-279.5"/><polygon fill="none" stroke="#002b36" points="190.5,-279.5 190.5,-296.5 210.5,-296.5 210.5,-279.5 190.5,-279.5"/><text text-anchor="start" x="193.5" y="-286.5" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#fdf6e3">a0</text></g><g id="edge3" class="edge"><title>140656891618224->140656891611744:n_trees</title><path fill="none" stroke="#002b36" d="M209.68-279.86C216.18-273.75 224.35-264.49 227.82-253.97"/><polygon fill="#002b36" stroke="#002b36" points="231.28,-254.44 229.5,-244 224.38,-253.28 231.28,-254.44"/></g><g id="edge4" class="edge"><title>140656891611744:output_0->140656891622544</title><path fill="none" stroke="#002b36" d="M110.5-200.5c-25.3.0-48.24 20.47-61.66 35.47"/><polygon fill="#002b36" stroke="#002b36" points="51.21,-162.42 42.09,-157.03 45.86,-166.93 51.21,-162.42"/></g><g id="edge8" class="edge"><title>140658408549984:output_0->140658408547392</title><path fill="none" stroke="#002b36" d="M69.5-53c0 8.42.0 17.76.0 25.62"/><polygon fill="#002b36" stroke="#002b36" points="73,-27.2 69.5,-17.2 66,-27.2 73,-27.2"/></g><g id="node8" class="node"><title>140656954282464</title><polygon fill="#268bd2" stroke="transparent" points="117.5,-350 117.5,-367 227.5,-367 227.5,-350 117.5,-350"/><polygon fill="none" stroke="#002b36" points="117.5,-350 117.5,-367 227.5,-367 227.5,-350 117.5,-350"/><text text-anchor="start" x="127" y="-357" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#fdf6e3">generate_data()</text><polygon fill="#fdf6e3" stroke="transparent" points="117.5,-333 117.5,-350 172.5,-350 172.5,-333 117.5,-333"/><polygon fill="none" stroke="#002b36" points="117.5,-333 117.5,-350 172.5,-350 172.5,-333 117.5,-333"/><text text-anchor="start" x="120.5" y="-340" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#002b36">output_0</text><polygon fill="#fdf6e3" stroke="transparent" points="172.5,-333 172.5,-350 227.5,-350 227.5,-333 172.5,-333"/><polygon fill="none" stroke="#002b36" points="172.5,-333 172.5,-350 227.5,-350 227.5,-333 172.5,-333"/><text text-anchor="start" x="175.5" y="-340" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="10" fill="#002b36">output_1</text></g><g id="edge10" class="edge"><title>140656954282464:output_1->140656954281072</title><path fill="none" stroke="#002b36" d="M228.5-341c14.09.0 13.6 19.15 11.07 34.1"/><polygon fill="#002b36" stroke="#002b36" points="242.99,-306.19 237.55,-297.1 236.14,-307.6 242.99,-306.19"/></g><g id="edge9" class="edge"><title>140656954282464:output_0->140656954280256</title><path fill="none" stroke="#002b36" d="M144.5-333C144.5-324.37 143.14-314.9 141.62-307"/><polygon fill="#002b36" stroke="#002b36" points="145.02,-306.18 139.49,-297.14 138.18,-307.65 145.02,-306.18"/></g></g></svg></figure><p>This data makes it possible to directly point to the variable <code>forest_acc</code>
and issue a query for all values in the storage that have the same qualitative
computational history:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>In</span> <span class=p>[</span><span class=mi>42</span><span class=p>]:</span> <span class=n>storage</span><span class=o>.</span><span class=n>similar</span><span class=p>(</span><span class=n>forest_acc</span><span class=p>,</span> <span class=n>context</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>Pattern</span><span class=o>-</span><span class=n>matching</span> <span class=n>to</span> <span class=n>the</span> <span class=n>following</span> <span class=n>computational</span> <span class=n>graph</span> <span class=p>(</span><span class=nb>all</span> <span class=n>constraints</span> <span class=n>apply</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>a0</span> <span class=o>=</span> <span class=n>Q</span><span class=p>()</span> <span class=c1># input to computation; can match anything</span>
</span></span><span class=line><span class=cl>    <span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>generate_data</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>trees</span> <span class=o>=</span> <span class=n>train_forest</span><span class=p>(</span><span class=n>X</span><span class=o>=</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=o>=</span><span class=n>y</span><span class=p>,</span> <span class=n>n_trees</span><span class=o>=</span><span class=n>a0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>forest_acc</span> <span class=o>=</span> <span class=n>eval_forest</span><span class=p>(</span><span class=n>trees</span><span class=o>=</span><span class=n>trees</span><span class=p>,</span> <span class=n>X</span><span class=o>=</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=o>=</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>result</span> <span class=o>=</span> <span class=n>storage</span><span class=o>.</span><span class=n>df</span><span class=p>(</span><span class=n>a0</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>trees</span><span class=p>,</span> <span class=n>forest_acc</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>Out</span><span class=p>[</span><span class=mi>42</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>   <span class=n>a0</span>  <span class=n>y</span>           <span class=n>X</span>                            <span class=n>trees</span>  <span class=n>forest_acc</span>
</span></span><span class=line><span class=cl><span class=mi>2</span>   <span class=mi>5</span>  <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=o>...</span>  <span class=p>[[</span><span class=mf>0.0</span><span class=p>,</span> <span class=mf>0.0</span><span class=p>,</span> <span class=o>...</span>  <span class=p>[</span><span class=n>DecisionTreeC</span><span class=o>...</span>        <span class=mf>0.96</span>
</span></span><span class=line><span class=cl><span class=mi>3</span>  <span class=mi>10</span>  <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=o>...</span>  <span class=p>[[</span><span class=mf>0.0</span><span class=p>,</span> <span class=mf>0.0</span><span class=p>,</span> <span class=o>...</span>  <span class=p>[</span><span class=n>DecisionTreeC</span><span class=o>...</span>        <span class=mf>0.99</span>
</span></span><span class=line><span class=cl><span class=mi>0</span>  <span class=mi>15</span>  <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=o>...</span>  <span class=p>[[</span><span class=mf>0.0</span><span class=p>,</span> <span class=mf>0.0</span><span class=p>,</span> <span class=o>...</span>  <span class=p>[</span><span class=n>DecisionTreeC</span><span class=o>...</span>        <span class=mf>0.99</span>
</span></span><span class=line><span class=cl><span class=mi>1</span>  <span class=mi>20</span>  <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=o>...</span>  <span class=p>[[</span><span class=mf>0.0</span><span class=p>,</span> <span class=mf>0.0</span><span class=p>,</span> <span class=o>...</span>  <span class=p>[</span><span class=n>DecisionTreeC</span><span class=o>...</span>        <span class=mf>0.99</span>
</span></span></code></pre></div><p>The <code>context=True</code> option includes the values <code>forest_acc</code> depends on in the
table. In particular, this reveals all the values of <code>n_trees</code> this workflow
was ran with, and the intermediate results along the way.</p><p>Finally, you have an explicit counterpart to this implicit query interface: if
you copy-paste the code for the computational graph above into a <code>with storage.query():</code> block, <code>result</code> will be the same table as above.</p><h3 id=what-just-happened->What just happened? 🤯<a hidden class=anchor aria-hidden=true href=#what-just-happened->#</a></h3><p>Behind the scenes, the computational graph that <code>forest_acc</code> is derived from
gets compiled to a SQL query over memoization tables. In the returned table,
each row is a matching of values to the variables that satisfies <em>all</em> the
computational relationships in the graph. This is the classical database concept
of <a href=https://en.wikipedia.org/wiki/Conjunctive_query>conjunctive queries</a>.</p><p>And here&rsquo;s (simplified) SQL code the pattern-matching query compiles to, where
you should think of <code>__objs__</code> as a table of all the Python objects in the storage:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-sql data-lang=sql><span class=line><span class=cl><span class=k>SELECT</span><span class=w> </span><span class=n>a0</span><span class=p>,</span><span class=w> </span><span class=n>y</span><span class=p>,</span><span class=w> </span><span class=n>X</span><span class=p>,</span><span class=w> </span><span class=n>trees</span><span class=p>,</span><span class=w> </span><span class=n>forest_acc</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>FROM</span><span class=w> 
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>__objs__</span><span class=p>.</span><span class=n>obj</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=n>a0</span><span class=p>,</span><span class=w> </span><span class=n>__objs__</span><span class=p>.</span><span class=n>obj</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=n>X</span><span class=p>,</span><span class=w> </span><span class=n>__objs__</span><span class=p>.</span><span class=n>obj</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=n>y</span><span class=p>,</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>__objs__</span><span class=p>.</span><span class=n>obj</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=n>trees</span><span class=p>,</span><span class=w> </span><span class=n>__objs__</span><span class=p>.</span><span class=n>obj</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=n>forest_acc</span><span class=p>,</span><span class=w> </span><span class=p>...</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>WHERE</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>generate_data</span><span class=p>.</span><span class=n>output_0</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>X</span><span class=w> </span><span class=k>and</span><span class=w> </span><span class=n>generate_data</span><span class=p>.</span><span class=n>output_1</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>y</span><span class=w> </span><span class=k>and</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>train_forest</span><span class=p>.</span><span class=n>X</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>X</span><span class=w> </span><span class=k>and</span><span class=w> </span><span class=n>train_forest</span><span class=p>.</span><span class=n>y</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>y</span><span class=w> </span><span class=k>and</span><span class=w> 
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>train_forest</span><span class=p>.</span><span class=n>a0</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>a0</span><span class=w> </span><span class=k>and</span><span class=w> </span><span class=n>train_forest</span><span class=p>.</span><span class=n>output_0</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>trees</span><span class=w> </span><span class=k>and</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>eval_forest</span><span class=p>.</span><span class=n>trees</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>trees</span><span class=w> </span><span class=k>and</span><span class=w> </span><span class=n>eval_forest</span><span class=p>.</span><span class=n>X</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>X</span><span class=w> </span><span class=k>and</span><span class=w> </span><span class=n>eval_forest</span><span class=p>.</span><span class=n>y</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>y</span><span class=w> </span><span class=k>and</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=n>eval_forest</span><span class=p>.</span><span class=n>output_0</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>forest_acc</span><span class=w>
</span></span></span></code></pre></div><h3 id=pattern-matching-with-collections-the-color-refinement-projection>Pattern-matching with collections: the color refinement projection<a hidden class=anchor aria-hidden=true href=#pattern-matching-with-collections-the-color-refinement-projection>#</a></h3><p>You can propagate more complex relationships through the declarative interface,
such as ones between a collection and its elements. This allows you to query
programs that combine multiple objects in a single result (e.g., data
aggregation, model ensembling, processing data in chunks, &mldr;) and/or generate a
variable number of objects (e.g., clustering, &mldr;).</p><p>To illustrate, suppose that just for fun we disrupt the nice flow of our program
by slicing into the list of trees to evaluate only on the first half:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>with</span> <span class=n>storage</span><span class=o>.</span><span class=n>run</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>generate_data</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>n_trees</span> <span class=ow>in</span> <span class=n>wrap</span><span class=p>((</span><span class=mi>10</span><span class=p>,</span> <span class=mi>15</span><span class=p>,</span> <span class=mi>20</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>        <span class=n>trees</span> <span class=o>=</span> <span class=n>train_forest</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>,</span> <span class=n>n_trees</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>forest_acc</span> <span class=o>=</span> <span class=n>eval_forest</span><span class=p>(</span><span class=n>trees</span><span class=p>[:</span><span class=n>n_trees</span> <span class=o>//</span> <span class=mi>2</span><span class=p>],</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span></code></pre></div><p>If you look at how <code>forest_acc</code> is computed now, you get a substantially larger
graph:</p><img src=../../expanded_graph.svg width=85% style=display:block;margin-left:auto;margin-right:auto><p>The problem is that there are now six (apparently, for <code>n_trees=20</code>, 12 or 13
trees survived the filtering!) chains of &ldquo;get an element from <code>trees</code>, put it in
a new list&rdquo; computations in this graph that are redundant (they look exactly the
same) and at the same time tie the graph to a particular value of <code>n_trees</code>.
This will make the query both slow and too specific for our needs.</p><p>There is a principled solution to eliminate such repetition from any
computational graph: a modified <a href=https://en.wikipedia.org/wiki/Colour_refinement_algorithm>color refinement
algorithm</a>. There&rsquo;s
too many details for the scope of this blog post, but the overall intuition
is that you can project<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> any computational graph to a
smaller graph where there are no two vertices that &ldquo;look the same&rdquo; in the
context of the entire computation<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. Here is what you get when you run <code>storage.draw_graph(forest_acc, project=True)</code>:</p><img src=../../projected_graph.svg width=50% style=display:block;margin-left:auto;margin-right:auto><p>And here is the code for this graph that you get using
<code>storage.print_graph(forest_acc, project=True)</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>idx0</span> <span class=o>=</span> <span class=n>Q</span><span class=p>()</span> <span class=c1># index into list</span>
</span></span><span class=line><span class=cl><span class=n>idx1</span> <span class=o>=</span> <span class=n>Q</span><span class=p>()</span> <span class=c1># index into list</span>
</span></span><span class=line><span class=cl><span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>generate_data</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>n_trees</span> <span class=o>=</span> <span class=n>Q</span><span class=p>()</span> <span class=c1># input to computation; can match anything</span>
</span></span><span class=line><span class=cl><span class=n>trees</span> <span class=o>=</span> <span class=n>train_forest</span><span class=p>(</span><span class=n>X</span><span class=o>=</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=o>=</span><span class=n>y</span><span class=p>,</span> <span class=n>n_trees</span><span class=o>=</span><span class=n>n_trees</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>a0</span> <span class=o>=</span> <span class=n>trees</span><span class=p>[</span><span class=n>idx1</span><span class=p>]</span> <span class=c1># a0 will match any element of a match for trees at index matching idx1</span>
</span></span><span class=line><span class=cl><span class=n>a1</span> <span class=o>=</span> <span class=n>ListQ</span><span class=p>(</span><span class=n>elts</span><span class=o>=</span><span class=p>[</span><span class=n>a0</span><span class=p>],</span> <span class=n>idxs</span><span class=o>=</span><span class=p>[</span><span class=n>idx0</span><span class=p>])</span> <span class=c1># a1 will match any list containing a match for a0 at index idx0</span>
</span></span><span class=line><span class=cl><span class=n>forest_acc</span> <span class=o>=</span> <span class=n>eval_forest</span><span class=p>(</span><span class=n>trees</span><span class=o>=</span><span class=n>a1</span><span class=p>,</span> <span class=n>X</span><span class=o>=</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=o>=</span><span class=n>y</span><span class=p>)</span>
</span></span></code></pre></div><p>Note how the two indices <code>idx0, idx1</code> are different variables. This means that
the result of this query will be larger than it intuitively needs to be, because
there will be matches for one index in <code>trees</code> and another index in
<code>trees[:n_trees//2]</code>. You can restrict it further by making <code>idx0, idx1</code> the
same variable.</p><h2 id=per-call-versioning-and-dependency-tracking>Per-call versioning and dependency tracking<a hidden class=anchor aria-hidden=true href=#per-call-versioning-and-dependency-tracking>#</a></h2><p>So far, we&rsquo;ve treated memoized functions as unchanging, but that&rsquo;s quite
unrealistic. One of the thorniest problems of data management is maintaining a
clear relationship between code and data in the face of changes in the internal
logic of a project&rsquo;s building blocks:</p><ul><li><strong>if you ignore a change in logic</strong>, you generally end up with a mixture of
results that <em>look</em> homogeneous to the system, but differ in the way they were
generated. This makes comparisons between them more or less meaningless.</li><li><strong>if every change is automatically marked as a change in logic</strong> (looking at
you, <code>git</code>), executions of
semantically equivalent code will be confusingly scattered across versions.
This makes comparisons between them difficult to program, and deters users
from code quality improvements.</li></ul><p>An automatic and universal solution to this problem is
<a href=https://stackoverflow.com/a/1132167/6538618>fundamentally impossible</a>. What
<code>mandala</code> offers instead is:</p><ul><li><strong>per-call dependency tracking</strong>: automatically track the functions and global
variables accessed by each memoized call, and alert you to changes in them, so
you can (carefully) choose whether a change to a dependency requires
recomputation of dependent calls</li><li><strong>content-addressable versions</strong>: use the current state of each dependency in
your codebase to automatically determine the currently compatible versions of
each memoized function to use in computation and queries. In particular, this
means that:<ul><li><strong>you can go &ldquo;back in time&rdquo;</strong> and access the storage relative to an earlier
state of the code (or even branch away in a new direction like in <code>git</code>) by
just restoring this state</li><li><strong>the code is the truth</strong>: when in doubt about the meaning of a result, you
can just look at the current code.</li></ul></li></ul><h3 id=warmup-adding-an-argument-backward-compatibly>Warmup: adding an argument backward-compatibly<a hidden class=anchor aria-hidden=true href=#warmup-adding-an-argument-backward-compatibly>#</a></h3><p>A very common type of change in practice is to have a function you&rsquo;ve run a few
times, and then get an idea for how it could do its work differently (e.g. use a
new algorithm, or vary a previously hardcoded constant). Importantly, you want
to keep the old results around, but also somehow distinguish them from the new
ones.</p><p>In <code>mandala</code>, you can do this by adding an argument <strong>with a default value</strong>,
and it JustWorks™. A column is added to the function&rsquo;s memoization table, with
the default value applied retroactively to past calls. All the memoized calls
without the new argument are still memoized. When doing this with a versioned
storage, you&rsquo;ll be shown a diff and should mark the change as <strong>not</strong> requiring
recomputation of dependents:</p><p><details open><summary markdown=span>Show/hide video</summary>
<video width=100% controls>
<source src=../../videos/add_arg.mp4 type=video/mp4><source src=../../videos/add_arg.webm type=video/webm>Your browser does not support HTML5 video.</video></details></p><h3 id=marking-changes-as-breaking-and-traveling-back-in-time>Marking changes as breaking and traveling back in time<a hidden class=anchor aria-hidden=true href=#marking-changes-as-breaking-and-traveling-back-in-time>#</a></h3><p>When you discover a bug affecting results, or just want to change how a function
works and recompute everything that depends on it, you mark the change as
requiring recomputation in the diff dialog. This will essentially tell the
storage that this function now &ldquo;means something else&rdquo; (under the hood, it has a
different <em>semantic id</em> from the previous version).</p><p>If you want to revisit the old results, restore the code to the old state; this
will cause the storage to interpret the function according to its previous
meaning in both computation and queries. You can examine the &ldquo;shallow&rdquo; versions
of a dependency (i.e. the revisions of its own source code) with
<code>storage.sources(f)</code>, and the &ldquo;deep&rdquo; versions (which include sets of
dependencies as seen by calls to this function) with <code>storage.versions(f)</code>:</p><p><details open><summary markdown=span>Show/hide video</summary>
<video width=100% controls>
<source src=../../videos/versioning.mp4 type=video/mp4><source src=../../videos/versioning.webm type=video/webm>Your browser does not support HTML5 video.</video></details></p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>This was a rather brisk walk through <code>mandala</code>, but it hopefully gave you an
idea of its general spirit and the particular ways in which it simplifies
computational data management. Some features that were not mentioned here, but
are currently in the works:</p><ul><li><strong>batch execution</strong>: separate data management and execution concerns in cases
when it&rsquo;s more efficient to run function calls in batches, such as inference on
large machine learning models. Code inside a <code>with storage.batch()</code> context is executed by a custom (batched) executor that you
can implement, yet each call is individually memoized and queriable.</li><li><strong>parallelization</strong>: since <code>mandala</code> is function-based, it&rsquo;s a great fit for
function-based parallelization frameworks like <a href=https://www.dask.org/><code>dask</code></a>.</li><li><strong>deletion interfaces</strong>: an imperative/declarative deletion interface,
analogous to the imperative/declarative query interfaces. Just change <code>with storage.run()</code> to <code>with storage.delete()</code>.</li></ul><p>We invite you to try <code>mandala</code> for your own projects and welcome contributions
to help improve its performance and capabilities!</p><h3 id=acknowledgements>Acknowledgements<a hidden class=anchor aria-hidden=true href=#acknowledgements>#</a></h3><p>Thanks to Stefan Krastanov, Melody Guan and Ben Kuhn for providing feedback on
drafts of this post.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>in the sense of a surjective labeled <a href=https://en.wikipedia.org/wiki/Graph_homomorphism>graph homomorphism</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>this works in almost all practical cases. However, there exist instances
in which the color refinement algorithm fails to distinguish two vertices in
the graph that do in fact have a differen role in the computation. When in
doubt, read the code for the computational graph printed during queries to
make sure it captures the relationships you want to capture.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>